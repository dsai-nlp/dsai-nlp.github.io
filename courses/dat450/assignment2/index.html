<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 2: Generating text from a language model | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment2/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-2-generating-text-from-a-language-model">DAT450/DIT247: Programming Assignment 2: Generating text from a language model</h1> <p>In this assignment, we extend the models we investigated in the previous assignment in two different ways:</p> <ul> <li>In the previous assignment, we used a model that takes a fixed number of previous words into account. Now, we will use a model capable of considering a variable number of previous words: a <em>recurrent neural network</em>. (Optionally, you can also investigate <em>Transformers</em>.)</li> <li>In this assignment, we will also use our language model to generate texts.</li> </ul> <h3 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h3> <ul> <li>Investigating more capable neural network architectures for language modeling.</li> <li>Understanding text-generating algorithms.</li> </ul> <h3 id="requirements">Requirements</h3> <p>Please submit your solution in <a href="https://chalmers.instructure.com/courses/31739/assignments/98455" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline</strong>: November 18.</p> <p>Submit a notebook containing your solution to the programming tasks described below. This is a pure programming assignment and you do not have to write a technical report or explain details of your solution in the notebook: there will be a separate individual assignment where you will answer some conceptual questions about what you have been doing here.</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>Make sure you have access to your solution for Programming Assignment 1 since you will reuse some parts.</p> <p>Copy the tokenization and integer encoding part into a new notebook.</p> <h2 id="step-1-adapting-your-code-for-rnns">Step 1: Adapting your code for RNNs</h2> <h3 id="adapting-the-preprocessing">Adapting the preprocessing</h3> <p>In the previous assignment, you developed preprocessing tools that extracted fixed-length sequences from the training data. You will now adapt the preprocessing so that you can deal with inputs of variable length.</p> <p><strong>Splitting</strong>: While we will deal with longer sequences than in the previous assignment, we’ll still have to control the maximal sequence length (or we’ll run out of GPU memory). Define a hyperparameter <code class="language-plaintext highlighter-rouge">max_sequence_length</code> and split your sequences into pieces that are at most of that length. (Side note: in RNN training, limiting the sequence length is called <a href="https://d2l.ai/chapter_recurrent-neural-networks/bptt.html" rel="external nofollow noopener" target="_blank"><em>truncated backpropagation through time</em></a>.)</p> <p><strong>Padding</strong>: In the previous assignment, you developed a tool that finds the most frequent words in order to build a vocabulary. In this vocabulary, you defined special symbols to cover a number of corner cases: the beginning and end of text passages, and when a word is previously unseen or too infrequent. Now, change your vocabulary builder to include a new special symbol that we will call <em>padding</em>: this will be used when our batches contain texts of different lengths.</p> <p>After these changes, preprocess the text and build the vocabulary as in the previous assignment. Store the integer-encoded paragraphs in two lists, corresponding to the training and validation sets.</p> <p><strong>Sanity check</strong>: You should have around 147,000 training paragraphs and 18,000 validation paragraphs. However, since you split the sequences, you will in the end get a larger number of training and validation instances. (The exact numbers depend on <code class="language-plaintext highlighter-rouge">max_sequence_length</code>.)</p> <h3 id="adapting-the-batcher">Adapting the batcher</h3> <p>In the previous assignment, you implemented some function to create training batches: that is, to put some number of training instances into a PyTorch tensor.</p> <p>Now, change your batching function so that it can deal with sequences of variable lengths. Since the output of the batching function are rectangular tensors, you need to <em>pad</em> sequences so they are of the same length. So for each instance that is shorter than the longest instance in the batch, you should append the padding symbol until it has the right length.</p> <p><strong>Sanity check</strong>: Inspect a few batches. Make sure that they are 2-dimensional integer tensors with <em>B</em> rows, where <em>B</em> is the batch size you defined. The number of columns probably varies from batch to batch, but should never be longer than <code class="language-plaintext highlighter-rouge">max_sequence_length</code> you defined previously. The integer-encoded padding symbol should only occur at the end of sequences.</p> <h2 id="step-2-designing-a-language-model-using-a-recurrent-neural-network">Step 2: Designing a language model using a recurrent neural network</h2> <h3 id="setting-up-the-neural-network-structure">Setting up the neural network structure</h3> <p>Define a neural network that implements an RNN-based language model. It should include the following layers:</p> <ul> <li>an <em>embedding layer</em> that maps token integers to floating-point vectors,</li> <li>an <em>recurrent layer</em> implementing some RNN variant (we suggest <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.LSTM</code></a> or <a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.GRU</code></a>),</li> <li>an <em>output layer</em> that computes (the logits of) a probability distribution over the vocabulary.</li> </ul> <p>You will have to define some hyperparameters such as the embedding size (as in the previous assignment) and the size of the RNN’s hidden state.</p> <details> <summary><b>Hint</b>: If you are doing the batching as recommended above, you should set <code>batch_first=True</code> when declaring the RNN.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> The input to an RNN is a 3-dimensional tensor. If we set <code>batch_first=True</code>, then we assume that the input tensor is arranged as (<em>B</em>, <em>N</em>, <em>E</em>) where <em>B</em> is the batch size, <em>N</em> is the sequence length, and <em>E</em> the embedding dimensionality. In this case, the RNN "walks" along the second dimension: that is, over the sequence of tokens. If on the other hand you set <code>batch_first=False</code>, then the RNN walks along the first dimension of the input tensor and it is assumed to be arranged as (<em>N</em>, <em>B</em>, <em>E</em>). </div> </details> <details> <summary><b>Hint</b>: How to apply RNNs in PyTorch.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> Take a look at the documentation of one of the RNN types in PyTorch. For instance, here is the documentation of <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="external nofollow noopener" target="_blank"><code>nn.LSTM</code></a>. In particular, look at the section called <b>Outputs</b>. It is important to note here that all types of RNNs return <b>two</b> outputs when you call them in the forward pass. In this assignment, you will need the <b>first</b> of these outputs, which correspond to the RNN's output for each <em>token</em>. (The other outputs are the <em>layer-wise</em> outputs.) </p> <p> As we discussed in the previous assignment, PyTorch allows users to set up neural networks in different ways: the more compact approach using <code>nn.Sequential</code>, and the more powerful approach by inheriting from <code>nn.Module</code>. </p> <p> If you implement your language model by inheriting from <code>nn.Module</code>, just remember that the RNN gives two outputs in the forward pass, and that you just need the first of them. </p> <pre>
class MyRNNBasedLanguageModel(nn.Module):
  def __init__(self, ... ):
    super().__init__()
    ... initialize model components here ...
    
  def forward(self, batch):
    embedded = ... apply the embedding layer ...
    rnn_out, _ = self.rnn(embedded)
    ... do the rest ...
</pre> <p> If you define your model using a <code>nn.Sequential</code>, we need a workaround to deal with the complication that the RNN returns two outputs. Here is one way to do it. </p> <pre>
class RNNOutputExtractor(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, rnn_out):
        return rnn_out[0]
</pre> <p> The <code>RNNOutputExtractor</code> can then be put after the RNN in your list of layers. </p> </div> </details> <p><strong>Sanity check</strong>: carry out the following steps:</p> <ul> <li>Create an integer tensor of shape 1x<em>N</em> where <em>N</em> is the length of the sequence. It doesn’t matter what the integers are except that they should be less than the vocabulary size. (Alternatively, take one instance from your training set.)</li> <li>Apply the model to this input tensor. It shouldn’t crash here.</li> <li>Make sure that the shape of the returned output tensor is 1x<em>N</em>x<em>V</em> where <em>V</em> is the size of the vocabulary. This output corresponds to the logits of the next-token probability distribution, but it is useless at this point because we haven’t yet trained the model.</li> </ul> <h3 id="training-the-model">Training the model</h3> <p>Adapt your training loop from the previous assignment, with the following changes</p> <details> <summary><b>Hint</b>: the output tensor is the input tensor, shifted one step to the right.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> For instance, let's say our training text is <em>This is great !</em> (in practice, the words will be integer-coded). That means that at the first word (<em>This</em>), we want the model to predict the second word (<em>is</em>). At the second word, the goal is to predict <em>great</em>, and so on. So when you process a batch in the training loop, you should probably split it into an input and an output part: <pre>
input_tokens = batch[:, :-1]
output_tokens = batch[:, 1:]
</pre> </div> This means that the input consists of all the columns in the batch except the last one, and the output of all the columns except the first one. </details> <details> <summary><b>Hint</b>: how to apply the loss function when training a language model.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> The loss function (<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank"><code>CrossEntropyLoss</code></a>) expects two input tensors: <ul> <li>the <em>logits</em> (that is: the unnormalized log probabilities) of the predictions,</li> <li>the <em>targets</em>, that is the true output values we want the model to predict.</li> </ul> Here, the tensor is expected to be one-dimensional (of length <em>B</em>, where <em>B</em> is the batch size) and the logits tensor to be two-dimensional (of shape (<em>B</em>, <em>V</em>) where <em>V</em> is the number of choices). In our case, the loss function's expected input format requires a small trick, since our targets tensor is two-dimensional (<em>B</em>, <em>N</em>) where <em>N</em> is the maximal text length in the batch. Analogously, the logits tensor is three-dimensional (<em>B</em>, <em>N</em>, <em>V</em>). To deal with this, you need to reshape the tensors before applying the loss function. <pre>
targets = targets.view(-1)                  # 2-dimensional -&gt; 1-dimensional
logits = logits.view(-1, logits.shape[-1])  # 3-dimensional -&gt; 2-dimensional
</pre> </div> </details> <details> <summary><b>Hint</b>: take padding into account when defining the loss.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> When the loss is computed, we don't want to include the positions where we have inserted the dummy padding tokens. <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank"><code>CrossEntropyLoss</code></a> has a parameter <code>ignore_index</code> that you can set to the integer you use to represent the padding tokens. </div> </details> <p>Run the training function and compute the perplexity on the validation set as in the previous assignment.</p> <h2 id="step-3-generating-text">Step 3: Generating text</h2> <h3 id="predicting-the-next-word">Predicting the next word</h3> <p>As a starting point, we’ll repeat the exercise from the first assignment where we see what the model predicts as the next word of a given sequence. For instance, for the sequence <code class="language-plaintext highlighter-rouge">he lives in san</code>, a well-trained model will typically predic the word <code class="language-plaintext highlighter-rouge">francisco</code>. The steps will typically be something like the following:</p> <ul> <li>Apply the model to the integer-encoded input text.</li> <li>Take the model’s output at the last position.</li> <li>Use <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html" rel="external nofollow noopener" target="_blank"><code>argmax</code></a> to find the index of the highest-scoring item.</li> <li>Apply the inverse vocabulary encoder (that you created in Step 2) so that you can understand what words the model thinks are the most likely in this context.</li> </ul> <h3 id="generating-texts">Generating texts</h3> <p>Implement a random sampling algorithm as described in the recording (<a href="https://youtu.be/QtwpM-OGOew" rel="external nofollow noopener" target="_blank">video</a>, <a href="http://www.cse.chalmers.se/~richajo/dat450/lectures/l4/m4_3.pdf" rel="external nofollow noopener" target="_blank">pdf</a>). The function should take the following inputs:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">model</code>: the language model that we use to predict the next token.</li> <li> <code class="language-plaintext highlighter-rouge">prompt</code>: the prompt that initializes the text generation.</li> <li> <code class="language-plaintext highlighter-rouge">max_length</code>: the maximal number of steps before terminating.</li> <li> <code class="language-plaintext highlighter-rouge">temperature</code>: controls the degree of randomness by scaling the predicted logits.</li> <li> <code class="language-plaintext highlighter-rouge">topk</code>: to implement top-K sampling, i.e. the next-token distribution is truncated so that it only includes the <code class="language-plaintext highlighter-rouge">topk</code> most probable tokens.</li> </ul> <p>The text generation should proceed until it an end-of-text symbol has been generated, or for at most <code class="language-plaintext highlighter-rouge">max_length</code> steps.</p> <details> <summary><b>Hint</b>: How to sample from the next-token distribution.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> The easiest option is probably to use <a href="https://pytorch.org/docs/stable/distributions.html#categorical" rel="external nofollow noopener" target="_blank"><code>torch.distributions.Categorical</code></a>. <code>Categorical</code> is a probability distribution over a set of choices, each of which has its own probability. So this is equivalent to the case where we have a set of possible next tokens, with different probabilities. </p> <p> The following code shows an example of how <code>Categorical</code> can be used. In your code, you will replace <code>example_logits</code> with the next-token distribution predicted by your language model. </p> <pre>
# Logits of the probabilities of 5 different choices.
example_logits = torch.tensor([0.0, 0.5, -0.2, 0.1, 0.05])
example_distr = Categorical(logits=example_logits)
sampled = example_distr.sample()
</pre> </div> </details> <details> <summary><b>Hint</b>: The <a href="https://pytorch.org/docs/stable/generated/torch.topk.html" rel="external nofollow noopener" target="_blank"><code>topk</code></a> function will be useful when you implement top-K sampling.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> This function takes a tensor as input and returns the <em>k</em> highest scores and their corresponding indices. </div> </details> <p><strong>Sanity check</strong>: There are two ways to make this random sampling algorithm behave like <em>greedy decoding</em> (that is: there is no randomness, and the most likely next word is selected in each step). Run the function in these two ways and make sure you get the same output in both cases.</p> <h2 id="optional-tasks">Optional tasks</h2> <p>These tasks can be done if you are curious but will not affect your score.</p> <h3 id="dealing-with-repetition">Dealing with repetition</h3> <p>As you might have observed, it is a common problem when generating from an autoregressive language model that some words or phrases are repeated over and over, in particular if you use greedy decoding (or beam search) or random sampling with a low temperature.</p> <p>Implement some trick to try to reduce the amount of repetition, for instance by penalizing the generation algorithm if it wants to generate words that it has already generated.</p> <h3 id="transformer-language-models">Transformer language models</h3> <p>Compare the RNN-based language model to an autoregressive Transformer. See the PyTorch tutorial for an example of how to set up a Transformer-based language model using PyTorch’s Transformer implementation.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>