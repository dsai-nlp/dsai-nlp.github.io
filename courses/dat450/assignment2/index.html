<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 2: Transformer language models | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="NLP@DSAI is a constellation of researchers who carry out foundational or applied research in natural language processing (NLP), or are interested in NLP techniques generally. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment2/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-2-transformer-language-models">DAT450/DIT247: Programming Assignment 2: Transformer language models</h1> <p>In this assignment, we extend the models we investigated in the previous assignment in two different ways:</p> <ul> <li>In the previous assignment, we used a model that takes a fixed number of previous words into account. Now, we will use a model capable of considering a variable number of previous words: a <em>recurrent neural network</em>. (Optionally, you can also investigate <em>Transformers</em>.)</li> <li>In this assignment, we will also use our language model to generate texts.</li> </ul> <h3 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h3> <ul> <li>Understanding the Transformer architecture in details, when used for language modeling.</li> <li>Understanding text-generating algorithms.</li> </ul> <h3 id="requirements">Requirements</h3> <p>Please submit your solution in <a href="https://chalmers.instructure.com/courses/XX/assignments/YY" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline</strong>: November XX.</p> <p>Submit a XX</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>Make sure you have access to your solution for Programming Assignment 1 since you will reuse some parts.</p> <p>Copy the skeleton from SOMEWHERE.</p> <h2 id="step-1-setting-up-a-transformer-neural-network">Step 1: Setting up a Transformer neural network</h2> <p><img src="https://raw.githubusercontent.com/ricj/dsai-nlp.github.io/refs/heads/master/_pages/dat450/olmo2_overview.svg" alt="Olmo2 overview" style="width:10%; height:auto;"></p> <p>To be fully compatible with the Olmo 2 implementation, note that all the <code class="language-plaintext highlighter-rouge">nn.Linear</code> inside of all layers are bias-free (<code class="language-plaintext highlighter-rouge">bias=False</code>). This includes Q, K, V, and O projections inside attention layers, all parts of the MLP layers, and the unembedding layer.</p> <h3 id="configuration">Configuration</h3> <h3 id="mlp-layer">MLP layer</h3> <p>Olmo 2 uses an MLP architecture called SwiGLU, which was introduced in <a href="https://arxiv.org/pdf/2002.05202" rel="external nofollow noopener" target="_blank">this paper</a>.</p> <p><strong>Sanity check.</strong></p> <h3 id="normalization">Normalization</h3> <p>To stabilize gradients during training, deep learning models with many layers often include some <em>normalization</em> (such as batch normalization or layer normalization). Transformers typically includes normalization layers at several places in the stack.</p> <p>Olmo 2 uses a type of normalization called <a href="https://arxiv.org/pdf/1910.07467" rel="external nofollow noopener" target="_blank">Root Mean Square layer normalization</a>.</p> <p>Here, you can either implement your own normalization layer, or use the built-in <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">RMSNorm</code></a> from PyTorch. In the PyTorch implementation, <code class="language-plaintext highlighter-rouge">eps</code> corresponds to <code class="language-plaintext highlighter-rouge">rms_norm_eps</code> from our model configuration, while <code class="language-plaintext highlighter-rouge">normalized_shape</code> should be equal to the hidden layer size. The hyperparameter <code class="language-plaintext highlighter-rouge">elementwise_affine</code> should be set to <code class="language-plaintext highlighter-rouge">True</code>, meaning that we include some learnable weights in this layer instead of a pure normalization.</p> <p>If you want to make your own layer, the PyTorch documentation shows the formula you will have to implement. (The $\gamma_i$ parameters are the learnable weights.)</p> <p><strong>Sanity check.</strong></p> <h3 id="multi-head-attention">Multi-head attention</h3> <p>Let’s take the trickiest part first!</p> <p>It is OK to use PyTorch’s <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code></a> to compute the final step. (In that case, set <code class="language-plaintext highlighter-rouge">is_causal=True</code>.)</p> <p>If you want to use your own implementation, the <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="external nofollow noopener" target="_blank">documentation of the PyTorch implementation</a> includes a piece of code that you can start from.</p> <p><strong>Sanity check.</strong></p> <h3 id="the-full-transformer-block">The full Transformer block</h3> <p><strong>Sanity check.</strong></p> <h3 id="the-complete-transformer-stack">The complete Transformer stack</h3> <p>The embedding and unembedding layers will be identical to what you had in Programming Assignment 1 (except that the unembedding layer should be bias-free, as mentioned above).</p> <h2 id="step-2-training-the-language-model">Step 2: Training the language model</h2> <p><strong>Alternative solution.</strong> Use a HuggingFace Trainer.</p> <p>Run the training function and compute the perplexity on the validation set as in the previous assignment.</p> <h2 id="step-3-generating-text">Step 3: Generating text</h2> <h3 id="predicting-the-next-word">Predicting the next word</h3> <p>As a starting point, we’ll repeat the exercise from the first assignment where we see what the model predicts as the next word of a given sequence. For instance, for the sequence <code class="language-plaintext highlighter-rouge">he lives in san</code>, a well-trained model will typically predict the word <code class="language-plaintext highlighter-rouge">francisco</code>. The steps will typically be something like the following:</p> <ul> <li>Apply the model to the integer-encoded input text.</li> <li>Take the model’s output at the last position (but make sure that you avoid an end-of-sentence dummy here).</li> <li>Use <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html" rel="external nofollow noopener" target="_blank"><code>argmax</code></a> to find the index of the highest-scoring item.</li> <li>Apply the inverse vocabulary encoder so that you can understand what words the model thinks are the most likely in this context.</li> </ul> <h3 id="generating-texts">Generating texts</h3> <p>Implement a random sampling algorithm as described in the recording (<a href="https://youtu.be/QtwpM-OGOew" rel="external nofollow noopener" target="_blank">video</a>, <a href="http://www.cse.chalmers.se/~richajo/dat450/lectures/l3/l3_generating.pdf" rel="external nofollow noopener" target="_blank">pdf</a>). The function should take the following inputs:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">model</code>: the language model that we use to predict the next token.</li> <li> <code class="language-plaintext highlighter-rouge">prompt</code>: the prompt that initializes the text generation.</li> <li> <code class="language-plaintext highlighter-rouge">max_length</code>: the maximal number of steps before terminating.</li> <li> <code class="language-plaintext highlighter-rouge">temperature</code>: controls the degree of randomness by scaling the predicted logits.</li> <li> <code class="language-plaintext highlighter-rouge">topk</code>: to implement top-K sampling, i.e. the next-token distribution is truncated so that it only includes the <code class="language-plaintext highlighter-rouge">topk</code> most probable tokens.</li> </ul> <p>The text generation should proceed until it an end-of-text symbol has been generated, or for at most <code class="language-plaintext highlighter-rouge">max_length</code> steps.</p> <details> <summary><b>Hint</b>: How to sample from the next-token distribution.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> The easiest option is probably to use <a href="https://pytorch.org/docs/stable/distributions.html#categorical" rel="external nofollow noopener" target="_blank"><code>torch.distributions.Categorical</code></a>. <code>Categorical</code> is a probability distribution over a set of choices, each of which has its own probability. So this is equivalent to the case where we have a set of possible next tokens, with different probabilities. </p> <p> The following code shows an example of how <code>Categorical</code> can be used. In your code, you will replace <code>example_logits</code> with the next-token distribution predicted by your language model. </p> <pre>
# Logits of the probabilities of 5 different choices.
example_logits = torch.tensor([0.0, 0.5, -0.2, 0.1, 0.05])
example_distr = Categorical(logits=example_logits)
sampled = example_distr.sample()
</pre> </div> </details> <details> <summary><b>Hint</b>: The <a href="https://pytorch.org/docs/stable/generated/torch.topk.html" rel="external nofollow noopener" target="_blank"><code>topk</code></a> function will be useful when you implement top-K sampling.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> This function takes a tensor as input and returns the <em>k</em> highest scores and their corresponding indices. </div> </details> <p>Run your generation algorithm with some different prompts and input parameters, and try to investigate the effects. In the reflection questions, you will be asked to summarize your impression of how texts are generated with different prompts and input parameters.</p> <p><strong>Sanity check</strong>: There are two ways to make this random sampling algorithm behave like <em>greedy decoding</em> (that is: there is no randomness, and the most likely next word is selected in each step). Run the function in these two ways and make sure you get the same output in both cases.</p> <h3 id="comparing-to-a-pre-trained-transformer">Comparing to a pre-trained Transformer</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import AutoTokenizer, AutoModelForCausalLM
local_dir = '/data/courses/2025_dat450_dit247/models/OLMo-2-0425-1B'
tokenizer = AutoTokenizer.from_pretrained(local_dir, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(local_dir, local_files_only=True)
</code></pre></div></div> <p>Note that this</p> <p><strong>Optional task.</strong> To verify that your implementation is identical to the Olmo 2 model, copy the weight tensors from the pre-trained model into an instance of your own implementation, and verify that you get exactly the same results.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?028b9776046506bdc87e88640817bd78"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>