<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 2: Transformer language models | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="NLP@DSAI is a constellation of researchers who carry out foundational or applied research in natural language processing (NLP), or are interested in NLP techniques generally. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment2/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-2-transformer-language-models">DAT450/DIT247: Programming Assignment 2: Transformer language models</h1> <p>In this assignment, we extend the models we investigated in the previous assignment in two different ways:</p> <ul> <li>We will now use a <em>Transformer</em> instead of the recurrent neural network we had previously.</li> <li>In this assignment, we will also use our language model to generate texts.</li> </ul> <h3 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h3> <ul> <li>Understanding the Transformer architecture in details, when used for language modeling.</li> <li>Understanding text-generating algorithms.</li> </ul> <h3 id="requirements">Requirements</h3> <p>Please submit your solution in <a href="https://chalmers.instructure.com/courses/XX/assignments/YY" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline</strong>: November 17.</p> <p>Submit a XX</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>Make sure you have access to your solution for Programming Assignment 1 since you will reuse the training loop. (Optionally, use HuggingFace’s <code class="language-plaintext highlighter-rouge">Trainer</code> instead.)</p> <p>Copy the skeleton from SOMEWHERE.</p> <h2 id="step-1-setting-up-a-transformer-neural-network">Step 1: Setting up a Transformer neural network</h2> <p>The main effort in this assignment is the reimplementation of a Transformer architecture. Specifically, we will mimic the architecture of the <a href="https://docs.allenai.org/release_notes/olmo-release-notes" rel="external nofollow noopener" target="_blank">OLMo 2</a> language model, released by the <a href="https://allenai.org/about" rel="external nofollow noopener" target="_blank">Allen AI institute</a> at the University of Washington.</p> <p>The figure below shows the design of the OLMo 2 Transformer. We will reimplement the MLP component and the multi-head attention (and optionally the normalizer as well), and then assemble all the pieces into a full Transformer stack.</p> <p><img src="https://raw.githubusercontent.com/ricj/dsai-nlp.github.io/refs/heads/master/_pages/dat450/olmo2_overview.svg" alt="Olmo2 overview" style="width:10%; height:auto;"></p> <p><strong>Implementation note:</strong> To be fully compatible with the OLMo 2 implementation, note that all the <code class="language-plaintext highlighter-rouge">nn.Linear</code> inside of all layers are bias-free (<code class="language-plaintext highlighter-rouge">bias=False</code>). This includes Q, K, V, and O projections inside attention layers, all parts of the MLP layers, and the unembedding layer. If you solve the optional task at the end where you copy the weights of a pre-trained model into your implementation, then it is important that all layers are identical in structure.</p> <h3 id="configuration">Configuration</h3> <h3 id="mlp-layer">MLP layer</h3> <p>OLMo 2 uses an MLP architecture called SwiGLU, which was introduced in <a href="https://arxiv.org/pdf/2002.05202" rel="external nofollow noopener" target="_blank">this paper</a>. (In the paper, this type of network is referred to as FFN<sub>SwiGLU</sub>, described on page 2, Equation 6. Swish<sub>1</sub> corresponds to PyTorch’s <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html" rel="external nofollow noopener" target="_blank">SiLU</a> activation.) The figure below shows the architecture visually; in the diagram, the ⊗ symbol refers to element-wise multiplication.</p> <p><img src="https://raw.githubusercontent.com/ricj/dsai-nlp.github.io/refs/heads/master/_pages/dat450/swiglu.svg" alt="SwiGLU" style="width:10%; height:auto;"></p> <p>The relevant hyperparameters you need to take into account here are <code class="language-plaintext highlighter-rouge">hidden_size</code> (the dimension of the input and output) and <code class="language-plaintext highlighter-rouge">intermediate_size</code> (the dimension of the intermediate representations).</p> <p><strong>Sanity check.</strong></p> <p>Create an untrained MLP layer. Create some 3-dimensional tensor where the last dimension has the same size as <code class="language-plaintext highlighter-rouge">hidden_size</code> in your MLP. If you apply the MLP to the test tensor, the output should have the same shape as the input.</p> <h3 id="normalization">Normalization</h3> <p>To stabilize gradients during training, deep learning models with many layers often include some <em>normalization</em> (such as batch normalization or layer normalization). Transformers typically includes normalization layers at several places in the stack. OLMo 2 uses a type of normalization called <a href="https://arxiv.org/pdf/1910.07467" rel="external nofollow noopener" target="_blank">Root Mean Square layer normalization</a>.</p> <p>You can either implement your own normalization layer, or use the built-in <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">RMSNorm</code></a> from PyTorch. In the PyTorch implementation, <code class="language-plaintext highlighter-rouge">eps</code> corresponds to <code class="language-plaintext highlighter-rouge">rms_norm_eps</code> from our model configuration, while <code class="language-plaintext highlighter-rouge">normalized_shape</code> should be equal to the hidden layer size. The hyperparameter <code class="language-plaintext highlighter-rouge">elementwise_affine</code> should be set to <code class="language-plaintext highlighter-rouge">True</code>, meaning that we include some learnable weights in this layer instead of a pure normalization.</p> <p>If you want to make your own layer, the PyTorch documentation shows the formula you should implement. (The $\gamma_i$ parameters are the learnable weights.)</p> <p><strong>Sanity check.</strong></p> <p>You can test this in the same way as you tested the MLP previously.</p> <h3 id="multi-head-attention">Multi-head attention</h3> <p>Now, let’s turn to the tricky part!</p> <p>The smaller versions of the OLMo 2 model, which we will follow here, use the same implementation of <em>multi-head attention</em> as the original Transformer, plus a couple of additional normalizers. (The bigger OLMo 2 models use <a href="https://sebastianraschka.com/llms-from-scratch/ch04/04_gqa/" rel="external nofollow noopener" target="_blank">grouped-query attention</a> rather than standard MHA; GQA is also used in various Llama, Qwen and some other popular LLMs.)</p> <p>The figure below shows what we will have to implement.</p> <p><strong>Hyperparameters:</strong> The hyperparameters you will need to consider when implementing the MHA are <code class="language-plaintext highlighter-rouge">hidden_size</code> which defines the input dimensionality as in the MLP and normalizer above, and <code class="language-plaintext highlighter-rouge">num_attention_heads</code> which defines the number of attention heads. <strong>Note</strong> that <code class="language-plaintext highlighter-rouge">hidden_size</code> has to be evenly divisible by <code class="language-plaintext highlighter-rouge">num_attention_heads</code>. (Below, we will refer to <code class="language-plaintext highlighter-rouge">hidden_size // num_attention_heads</code> as the head dimensionality $d_h$.)</p> <p><strong>Defining MHA components.</strong> In <code class="language-plaintext highlighter-rouge">__init__</code>, define the <code class="language-plaintext highlighter-rouge">nn.Linear</code> components (square matrices) that compute query, key, and value representations, and the final outputs. (They correspond to what we called $W_Q$, $W_K$, $W_V$, and $W_O$ in <a href="https://www.cse.chalmers.se/~richajo/dat450/lectures/l4/m4_2.pdf" rel="external nofollow noopener" target="_blank">the lecture on Transformers</a>.) OLMo 2 also applies layer normalizers after the query and key representations.</p> <p><strong>MHA computation, step 1.</strong> The <code class="language-plaintext highlighter-rouge">forward</code> method takes two inputs <code class="language-plaintext highlighter-rouge">hidden_states</code> and <code class="language-plaintext highlighter-rouge">position_embedding</code>.</p> <p>Continuing to work in <code class="language-plaintext highlighter-rouge">forward</code>, now compute query, key, and value representations; don’t forget the normalizers after the query and key representations.</p> <p>Now, we need to reshape the query, key, and value tensors so that the individual attention heads are stored separately. Assume your tensors have the shape \((b, m, d)\), where \(b\) is the batch size, \(m\) the text length, and \(d\) the hidden layer size. We now need to reshape and transpose so that we get \((b, n_h, m, d_h)\) where \(n_h\) is the number of attention heads and \(d_h\) the attention head dimensionality. Your code could be something like the following (apply this to queries, keys, and values):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>q = q.view(b, m, n_h, d_h).transpose(1, 2)
</code></pre></div></div> <p>Now apply the RoPE rotations to the query and key representations. Use the utility function <code class="language-plaintext highlighter-rouge">apply_rotary_pos_emb</code> provided in the code skeleton and just provide the <code class="language-plaintext highlighter-rouge">position_embedding</code> that you received as an input to <code class="language-plaintext highlighter-rouge">forward</code>. The utility function returns the modified query and key representations.</p> <p><strong>Sanity check step 1.</strong> Create an untrained MHA layer. Create some 3-dimensional tensor where the last dimension has the same size as <code class="language-plaintext highlighter-rouge">hidden_size</code>, as you did in the previous sanity checks. Apply the MHA layer with what you have implemented so far and make sure it does not crash. (It is common to see errors related to tensor shapes here.)</p> <p><strong>MHA computation, step 2.</strong> Now, implement the attention mechanism itself. We will explain the exact computations in the hint below, but conveniently enough PyTorch’s <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code></a> (with <code class="language-plaintext highlighter-rouge">is_causal=True</code>) implements everything that we have to do here. Optionally, implement your own solution.</p> <details> <summary><b>Hint</b>: Some advice if you want to implement your own attention.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> In that case, the <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="external nofollow noopener" target="_blank">documentation of the PyTorch implementation</a> includes a piece of code that can give you some inspiration and that you can simplify somewhat. Assuming your query, key, and value tensors are called $$q$$, $$k$$, and $$v$$, then the computations you should carry out are the following. First, we compute the <em>attention pre-activations</em>, which are compute by multiplying query and key representations, and scaling: $$ \alpha(q, k) = \frac{q \cdot k^{\top}}{\sqrt{d_h}} $$ The transposition of the key tensor can be carried out by calling <code>k.transpose(-2, -1)</code>. Second, add a *causal mask* to the pre-activations. This mask is necessary for autoregressive (left-to-right) language models: this is so that the attention heads can only consider tokens before the current one. The mask should have the shape \((m, m)\); its lower triangle including the diagonal should be 0 and the upper triangle \(-\infty\). Pytorch's <a href="https://docs.pytorch.org/docs/stable/generated/torch.tril.html" rel="external nofollow noopener" target="_blank"><code>tril</code></a> or <a href="https://docs.pytorch.org/docs/stable/generated/torch.triu.html" rel="external nofollow noopener" target="_blank"><code>triu</code></a> can be convenient here. Then apply the softmax to get the attention weights. $$ A(q, k) = \text{softmax}(\alpha(q, k) + \text{mask}) $$ Finally, multiply the attention weights by the value tensor and return the result. $$ \text{Attention}(q, k, v) = A(q, k) \cdot v $$ </div> </details> <p><strong>MHA computation, step 3.</strong> Now, we need to combine the results from the individual attention heads. We first flip the second and third dimensions of the tensor (so that the first two dimensions correspond to the batch length and text length), and then reshape into the right shape.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>attn_out = attn_out.transpose(1, 2).reshape(b, m, d)
</code></pre></div></div> <p>Then compute the final output representation (by applying the linear layer we called (W_O) above) and return the result.</p> <p><strong>Sanity check steps 2 and 3.</strong> Once again create a MHA layer for testing and apply it to an input tensor of the same shape as before. Assuming you don’t get any crashes here, the output should be of the same shape as the input. If it crashes or your output has the wrong shape, insert <code class="language-plaintext highlighter-rouge">print</code> statements along the way, or use an editor with step-by-step debugging, to check the shapes at each step.</p> <h3 id="the-full-transformer-decoder-layer">The full Transformer decoder layer</h3> <p>After coding up the multi-head attention, everything else is just a simple assembly of pieces!</p> <p>In the constructor <code class="language-plaintext highlighter-rouge">__init__</code>, create the components in this block, taking the model configuration into account. As shown in the figure, a Transformer layer should include an attention layer and an MLP, with normalizers. In <code class="language-plaintext highlighter-rouge">forward</code>, connect the components to each other; remember to put residual connections at the right places.</p> <details> <summary><b>Hint</b>: Residual connections in PyTorch.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> Assuming your <pre>
h_new = do_something(h_old) 
out = h_new + h_old
</pre> </div> </details> <p><strong>Sanity check.</strong> Carry out the usual sanity check to see that the shapes are right and there are no crashes.</p> <h3 id="the-complete-transformer-stack">The complete Transformer stack</h3> <p>Now, set up the complete Transformer stack including embedding and unembedding layers. The embedding and unembedding layers will be identical to what you had in Programming Assignment 1 (except that the unembedding layer should be bias-free, as mentioned in the beginning).</p> <details> <summary><b>Hint</b>: Use a <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html" rel="external nofollow noopener" target="_blank"><code>ModuleList</code></a>.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> Put all the Transformer blocks in a <code>ModuleList</code> instead of a plain Python list. The <code>ModuleList</code> makes sure your parameters are registered so that they are included when you compute the gradients. &lt;/pre&gt; </div> </details> <details> <summary><b>Hint</b>: Creating the RoPE embeddings.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> Xxx. &lt;/pre&gt; </div> </details> <h2 id="step-2-training-the-language-model">Step 2: Training the language model</h2> <p>In Assignment 1, you implemented a utility to handle training and validation. Your Transformer language model should be possible to use as a drop-in replacement for the RNN-based model you had in that assignment.</p> <p><strong>Alternative solution.</strong> Use a HuggingFace Trainer.</p> <p>Select some suitable hyperparameters (number of Transformer layers, hidden layer size, number of attention heads). Then run the training function and compute the perplexity on the validation set as in the previous assignment.</p> <h2 id="step-3-generating-text">Step 3: Generating text</h2> <h3 id="predicting-the-next-word">Predicting the next word</h3> <p>As a starting point, we’ll repeat the exercise from the first assignment where we see what the model predicts as the next word of a given sequence. For instance, for the sequence <code class="language-plaintext highlighter-rouge">he lives in san</code>, a well-trained model will typically predict the word <code class="language-plaintext highlighter-rouge">francisco</code>. The steps will typically be something like the following:</p> <ul> <li>Apply the model to the integer-encoded input text.</li> <li>Take the model’s output at the last position (but make sure that you avoid an end-of-sentence dummy here).</li> <li>Use <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html" rel="external nofollow noopener" target="_blank"><code>argmax</code></a> to find the index of the highest-scoring item.</li> <li>Apply the inverse vocabulary encoder so that you can understand what words the model thinks are the most likely in this context.</li> </ul> <h3 id="generating-texts">Generating texts</h3> <p>Implement a random sampling algorithm as described in the recording (<a href="https://youtu.be/QtwpM-OGOew" rel="external nofollow noopener" target="_blank">video</a>, <a href="http://www.cse.chalmers.se/~richajo/dat450/lectures/l3/l3_generating.pdf" rel="external nofollow noopener" target="_blank">pdf</a>). The function should take the following inputs:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">model</code>: the language model that we use to predict the next token.</li> <li> <code class="language-plaintext highlighter-rouge">prompt</code>: the prompt that initializes the text generation.</li> <li> <code class="language-plaintext highlighter-rouge">max_length</code>: the maximal number of steps before terminating.</li> <li> <code class="language-plaintext highlighter-rouge">temperature</code>: controls the degree of randomness by scaling the predicted logits.</li> <li> <code class="language-plaintext highlighter-rouge">topk</code>: to implement top-K sampling, i.e. the next-token distribution is truncated so that it only includes the <code class="language-plaintext highlighter-rouge">topk</code> most probable tokens.</li> </ul> <p>The text generation should proceed until it an end-of-text symbol has been generated, or for at most <code class="language-plaintext highlighter-rouge">max_length</code> steps.</p> <details> <summary><b>Hint</b>: How to sample from the next-token distribution.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> The easiest option is probably to use <a href="https://pytorch.org/docs/stable/distributions.html#categorical" rel="external nofollow noopener" target="_blank"><code>torch.distributions.Categorical</code></a>. <code>Categorical</code> is a probability distribution over a set of choices, each of which has its own probability. So this is equivalent to the case where we have a set of possible next tokens, with different probabilities. </p> <p> The following code shows an example of how <code>Categorical</code> can be used. In your code, you will replace <code>example_logits</code> with the next-token distribution predicted by your language model. </p> <pre>
# Logits of the probabilities of 5 different choices.
example_logits = torch.tensor([0.0, 0.5, -0.2, 0.1, 0.05])
example_distr = Categorical(logits=example_logits)
sampled = example_distr.sample()
</pre> </div> </details> <details> <summary><b>Hint</b>: The <a href="https://pytorch.org/docs/stable/generated/torch.topk.html" rel="external nofollow noopener" target="_blank"><code>topk</code></a> function will be useful when you implement top-K sampling.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> This function takes a tensor as input and returns the <em>k</em> highest scores and their corresponding indices. </div> </details> <p>Run your generation algorithm with some different prompts and input parameters, and try to investigate the effects. In the reflection questions, you will be asked to summarize your impression of how texts are generated with different prompts and input parameters.</p> <p><strong>Sanity check</strong>: There are two ways to make this random sampling algorithm behave like <em>greedy decoding</em> (that is: there is no randomness, and the most likely next word is selected in each step). Run the function in these two ways and make sure you get the same output in both cases.</p> <h3 id="comparing-to-a-pre-trained-transformer">Comparing to a pre-trained Transformer</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import AutoTokenizer, AutoModelForCausalLM
local_dir = '/data/courses/2025_dat450_dit247/models/OLMo-2-0425-1B'
tokenizer = AutoTokenizer.from_pretrained(local_dir, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(local_dir, local_files_only=True)
</code></pre></div></div> <p>Note that this</p> <p><strong>Optional task.</strong> To verify that your implementation is identical to the Olmo 2 model, copy the weight tensors from the pre-trained model into an instance of your own implementation, and verify that you get exactly the same results.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?028b9776046506bdc87e88640817bd78"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>