<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 1: Introduction to language modeling | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="NLP@DSAI is a constellation of researchers who carry out foundational or applied research in natural language processing (NLP), or are interested in NLP techniques generally. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment1/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-1-introduction-to-language-modeling">DAT450/DIT247: Programming Assignment 1: Introduction to language modeling</h1> <p><em>Language modeling</em> is the foundation that recent advances in NLP technlogies build on. In essence, language modeling means that we learn how to imitate the language that we observe in the wild. More formally, we want to train a system that models the statistical distribution of natural language. Solving this task is exactly what the famous commercial large language models do (with some additional post-hoc tweaking to make the systems more interactive and avoid generating provocative outputs).</p> <p>In the course, we will cover a variety of technical solutions to this fundamental task (e.g. recurrent models and Transformers). In this first assignment of the course, we are going to build a neural network-based language model using simple techniques that should be familiar to anyone with an experience of neural networks. In essence, our solution is going to be similar to the one described by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">Bengio et al. (2003)</a>.</p> <h3 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h3> <ul> <li>Introducing the task of language modeling</li> <li>Getting experience of preprocessing text</li> <li>Understanding the concept of word embeddings</li> <li>Refreshing basic skills in how to set up and train a neural network</li> </ul> <h3 id="requirements">Requirements</h3> <p>Please submit your solution in <a href="https://chalmers.instructure.com/courses/31739/assignments/98342" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline</strong>: November 11.</p> <p>Submit a notebook containing your solution to the programming tasks described below. This is a pure programming assignment and you do not have to write a technical report or explain details of your solution in the notebook: there will be a separate individual assignment where you will answer some conceptual questions about what you have been doing here.</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>If you are working on your own machine, make sure that the following libraries are installed:</p> <ul> <li> <a href="https://www.nltk.org/install.html" rel="external nofollow noopener" target="_blank">NLTK</a> or <a href="https://spacy.io/usage" rel="external nofollow noopener" target="_blank">SpaCy</a> for tokenization,</li> <li> <a href="https://pytorch.org/get-started/locally/" rel="external nofollow noopener" target="_blank">PyTorch</a> for building and training the models,</li> <li>Optional: <a href="https://matplotlib.org/stable/users/getting_started/" rel="external nofollow noopener" target="_blank">Matplotlib</a> and <a href="https://scikit-learn.org/stable/install.html" rel="external nofollow noopener" target="_blank">scikit-learn</a> for the embedding visualization in the last step. If you are using a Colab notebook, these libraries are already installed.</li> </ul> <p>For the third part of the assignment, you will need to understand some basic concepts of PyTorch such as tensors, models, optimizers, loss functions and how to write the training loop. There are plenty of tutorials available, for instance on the <a href="https://pytorch.org/tutorials/" rel="external nofollow noopener" target="_blank">PyTorch website</a>. From the Applied Machine Learning course, there is also an <a href="https://www.cse.chalmers.se/~richajo/dit866/lectures/l7/Implementing%20classifiers%20with%20PyTorch.html" rel="external nofollow noopener" target="_blank">example notebook</a> that shows how to train a basic classifier in PyTorch. (But note that if you take code from this notebook, several technical details have to change since our input data and prediction task are different!)</p> <h2 id="step-1-preprocessing-the-text">Step 1: Preprocessing the text</h2> <p>Download and extract <a href="https://www.cse.chalmers.se/~richajo/diverse/lmdemo.zip" rel="external nofollow noopener" target="_blank">this archive</a>, which contains three text files. The files have been created from Wikipedia articles converted into raw text, with all Wiki markup removed. (We’ll actually just use the training and validation sets, and you can ignore the test file.)</p> <p>You will need a <em>tokenizer</em> that splits English text into separate words (tokens). In this assignment, you will just use an existing tokenizer. Popular NLP libraries such as SpaCy and NLTK come with built-in tokenizers. We recommend NLTK in this assignment since it is somewhat faster than SpaCy and somewhat easier to use.</p> <details> <summary><b>Hint</b>: How to use NLTK's English tokenizer.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;">Import the function <code>word_tokenize</code> from the <code>nltk</code> library. If you are running this on your own machine, you will first need to install NLTK with <code>pip</code> or <code>conda</code>. In Colab, NLTK is already installed. For instance, <code>word_tokenize("Let's test!!")</code> should give the result <code>["Let", "'s", "test", "!", "!"]</code> </div> </details> <p>Each nonempty line in the text files correspond to one paragraph in Wikipedia. Apply the tokenizer to all paragraphs in the training and validation datasets. Convert all words into lowercase.</p> <p><strong>Sanity check</strong>: after this step, your training set should consist of around 147,000 paragraphs and the validation set around 18,000 paragraphs. (The exact number depends on what tokenizer you selected.)</p> <h2 id="step-2-encoding-the-text-as-integers">Step 2: Encoding the text as integers</h2> <h3 id="building-the-vocabulary">Building the vocabulary</h3> <p>Create a utility (a function or a class) that goes through the training text and creates a <em>vocabulary</em>: a mapping from token strings to integers.</p> <p>In addition, the vocabulary should contain 3 special symbols:</p> <ul> <li>a symbol for previously unseen or low-frequency tokens,</li> <li>a symbol we will put at the beginning of each paragraph,</li> <li>a symbol we will put at the end of each paragraph.</li> </ul> <p>The total size of the vocabulary (including the 3 symbols) should be at most <code class="language-plaintext highlighter-rouge">max_voc_size</code>, which is is a user-specified hyperparameter. If the number of unique tokens in the text is greater than <code class="language-plaintext highlighter-rouge">max_voc_size</code>, then use the most frequent ones.</p> <details> <summary><b>Hint</b>: A <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="external nofollow noopener" target="_blank"><code>Counter</code></a> can be convenient when computing the frequencies.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;">A <code>Counter</code> is like a regular Python dictionary, with some additional functionality for computing frequencies. For instance, you can go through each paragraph and call <a href="https://docs.python.org/3/library/collections.html#collections.Counter.update" rel="external nofollow noopener" target="_blank"><code>update</code></a>. After building the <code>Counter</code> on your dataset, <a href="https://docs.python.org/3/library/collections.html#collections.Counter.most_common" rel="external nofollow noopener" target="_blank"><code>most_common</code></a> gives the most frequent items.</div> </details> <p>Also create some tool that allows you to go back from the integer to the original word token. This will only be used in the final part of the assignment, where we look at model outputs and word embedding neighbors.</p> <p><strong>Example</strong>: you might end up with something like this:</p> <pre>
str_to_int = { 'BEGINNING':0, 'END':1, 'UNKNOWN':2, 'the':3, 'and':4, ... }

int_to_str = { 0:'BEGINNING', 1:'END', 2:'UNKNOWN', 3:'the', 4:'and', ... }
</pre> <p><strong>Sanity check</strong>: after creating the vocabulary, make sure that</p> <ul> <li>the size of your vocabulary is not greater than the max vocabulary size you specified,</li> <li>the 3 special symbols exist in the vocabulary and that they don’t coincide with any real words,</li> <li>some highly frequent example words (e.g. “the”, “and”) are included in the vocabulary but that some rare words (e.g. “cuboidal”, “epiglottis”) are not.</li> <li>if you take some test word, you can map it to an integer and then back to the original test word using the inverse mapping.</li> </ul> <h3 id="encoding-the-texts-and-creating-training-instances">Encoding the texts and creating training instances</h3> <p>The model we are going to train will predict the next token given the previous <em>N</em> tokens. We will now create the examples we will use for training and evaluation by extracting word sequences from the provided texts.</p> <p>First, make an educated guess about the size of the context window <em>N</em> you are going to use. (You can go back later on and try out other values.) Any value of <em>N</em> greater than 0 should work for the purposes of this assignment. Intuitively, small context windows (e.g. <em>N</em>=1) makes the model more stupid but a bit more efficient in terms of time and memory.</p> <p>Go through the training and validation data and extract all sequences of <em>N</em>+1 tokens and map them to the corresponding integer values. Remember to use the special symbols when necessary:</p> <ul> <li>the “unseen” symbol for tokens not in your vocabulary,</li> <li> <em>N</em> “beginning” symbols before each paragraph,</li> <li>an “end” symbol after each paragraph.</li> </ul> <p>Store all these sequences in lists.</p> <p><strong>Example</strong>: If our training text consists of the three tokens <em>Wonderful news !</em>, and we use a context window size of 1, we would extract the training instances <code class="language-plaintext highlighter-rouge">[['BEGINNING', 'wonderful'], ['wonderful', 'news'], ['news', '!'], ['!', 'END']]</code>. After integer encoding, we might have something like <code class="language-plaintext highlighter-rouge">[[0, 2], [2, 3], [3, 4], [4, 1]]</code>, depending on how our encoding works.</p> <p><strong>Sanity check</strong>: after these steps, you should have around 12 million training instances and 1.5 million validation instances.</p> <h2 id="step-3-developing-a-language-model">Step 3: Developing a language model</h2> <h3 id="creating-training-batches">Creating training batches</h3> <p>When using neural networks, we typically process several instances at the time when training the model and when running the trained model. Make sure that you have a way to put your training instances in batches of a fixed size. It is enough to go through the training instances with a <code class="language-plaintext highlighter-rouge">for</code> loop, taking steps of length <code class="language-plaintext highlighter-rouge">batch_size</code>, but most solutions would probably use a <code class="language-plaintext highlighter-rouge">DataLoader</code> here. Put each batch in a PyTorch tensor (e.g. by calling <code class="language-plaintext highlighter-rouge">torch.as_tensor</code>).</p> <details> <summary><b>Hint</b>: More information about <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="external nofollow noopener" target="_blank"><code>DataLoader</code></a>.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> PyTorch provides a utility called <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="external nofollow noopener" target="_blank"><code>DataLoader</code></a> to help us create batches. It can work on a variety of underlying data structures, but in this assignment, we'll just apply it to the list you prepared previously. <pre>
dl = DataLoader(your_list, batch_size=..., shuffle=..., collate_fn=torch.as_tensor)
</pre> The arguments here are as follows: <ul> <li> <code>batch_size</code>: the number of instances in each batch.</li> <li> <code>shuffle</code>: whether or not we rearrange the instances randomly. It is common to shuffle instances while training.</li> <li> <code>collate_fn</code>: a function that defines how each batch is created. In our case, we just want to put each batch in a tensor.</li> </ul> When you have created a <code>DataLoader</code>, you can iterate through the dataset batch by batch: <pre>for batch in dl:
   ... do something with each batch ...</pre> </div> </details> <p><strong>Sanity check</strong>: Make sure that your batches are PyTorch tensors of shape (<em>B</em>, <em>N</em>+1) where <em>B</em> is the batch size and <em>N</em> the number of context tokens. (Depending on your batch size, the last batch in the training set might be smaller than <em>B</em>.) Each batch should contain integers, not floating-point numbers.</p> <h3 id="setting-up-the-neural-network-structure">Setting up the neural network structure</h3> <p>Set up a neural network inspired by the neural language model proposed by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">Bengio et al. (2003)</a>. The main components are:</p> <ul> <li>an <em>embedding layer</em> that maps token integers to floating-point vectors,</li> <li> <em>intermediate layers</em> that map between input and output representations,</li> <li>an <em>output layer</em> that computes (the logits of) a probability distribution over the vocabulary.</li> </ul> <p>You are free to experiment with the design of the intermediate layers and you don’t have to follow the exact structure used in the paper.</p> <details> <summary><b>Hint</b>: Setting up a neural network in PyTorch.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;">There are a few different ways that we can write code to set up a neural network in PyTorch. If your model has the traditional structure of stacked layers, then the most concise way to declare the model is to use <code>nn.Sequential</code>: <pre>
model = nn.Sequential(
  layer1,
  layer2,
  ...
  layerN)
</pre> You can use any type of layers here. In our case, you'll typically start with a <code>nn.Embedding</code> layer, followed by some intermediate layers (e.g. <code>nn.Linear</code> followed by some activation such as <code>nn.ReLU</code>), and then a linear output layer. A more general solution is to declare your network as a class that inherits from <code>nn.Module</code>. You will then have to declare your model components in <code>__init__</code> and define the forward computation in `forward`: <pre>
class MyNetwork(nn.Module):
  def __init__(self, hyperparameters):
    super().__init__()
    self.layer1 = ... some layer ...
    self.layer2 = ... some layer ...
    ...

  def forward(self, inputs):
    step1 = self.layer1(inputs)
    step2 = self.layer2(step1)
    ...
    return something
</pre> The second coding style, while more verbose, has the advantage that it is easier to debug: for instance, it is easy to check the shapes of intermediate computations. It is also more flexible and allows you to go beyond the constraints of a traditional layered setup.</div> </details> <details> <summary><b>Hint</b>: It can be useful to put a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" rel="external nofollow noopener" target="_blank"><code>nn.Flatten</code></a> after the embedding layer.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" rel="external nofollow noopener" target="_blank"><code>nn.Flatten</code></a> is a convenient tool that you can put after the embedding layer to get the right tensor shapes. Let's say we have a batch of <em>B</em> inputs, each of which is a context window of size <em>N</em>, so our input tensor has the shape (<em>B</em>, <em>N</em>). The output from the embedding layer will then have the shape (<em>B</em>, <em>N</em>, <em>D</em>) where <em>D</em> is the embedding dimensionality. If you use a <code>nn.Flatten</code>, we go back to a two-dimensional tensor of shape (<em>B</em>, <em>N</em>*<em>D</em>). That is, we can see this as a step that concatenates the embeddings of the tokens in the context window.</div> </details> <p><strong>Sanity check</strong>: carry out the following steps:</p> <ul> <li>Create an integer tensor of shape 1x<em>N</em> where <em>N</em> is the size of the context window. It doesn’t matter what the integers are except that they should be less than the vocabulary size. (Alternatively, take one instance from your training set.)</li> <li>Apply the model to this input tensor. It shouldn’t crash here.</li> <li>Make sure that the shape of the returned output tensor is 1x<em>V</em> where <em>V</em> is the size of the vocabulary. This output corresponds to the logits of the next-token probability distribution, but it is useless at this point because we haven’t yet trained the model.</li> </ul> <h3 id="training-the-model">Training the model</h3> <p>Before training, we need two final pieces:</p> <ul> <li>A <em>loss</em> that we want to minimize. In our case, this is going to be the cross-entropy loss, implemented in PyTorch as <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank"><code>nn.CrossEntropyLoss</code></a>. Minimizing this loss corresponds to minimizing the negative log probability of the observed tokens.</li> <li>An <em>optimizer</em> that updates model parameters, based on the gradients with respect to the model parameters. The optimizer typically implements some variant of <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="external nofollow noopener" target="_blank">stochastic gradient descent</a>. An example of a popular optimizer is <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" rel="external nofollow noopener" target="_blank"><code>AdamW</code></a>.</li> </ul> <p>Now, we are ready to train the neural network on the training set. Using the loss, the optimizer, and the training batches, write a <em>training loop</em> that iterates through the batches and updates the model incrementally to minimize the loss.</p> <p>If you followed our previous implementation advice, your training batches will be integer tensors of shape (<em>B</em>, <em>N</em>+1) where <em>B</em> is the batch size and <em>N</em> is the context window size; the first <em>N</em> columns correspond to the context windows and the last column the tokens we are predicting. So while you are training, you will apply the model to the first <em>N</em> columns of the batch and compute the loss with respect to the last column.</p> <details> <summary><b>Hint</b>: A typical PyTorch training loop.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <pre>
for each training epoch:
    for each batch B in the training set:
        FORWARD PASS:
	X = first N columns in B
	Y = last column in B
	put X and Y on the GPU
        apply the model to X
	compute the loss for the model output and Y
        BACKWARD PASS (updating the model parameters):
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()	
</pre> </div> </details> <details> <summary><b>Hint</b>: Some advice on development.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> While developing the code, work with very small datasets until you know it doesn't crash, and then use the full training set. Monitor the cross-entropy loss (and/or the perplexity) over the training: if the loss does not decrease while you are training, there is probably an error. For instance, if the learning rate is set to a value that is too large, the loss values may be unstable or increase. </div> </details> <h2 id="step-4-evaluation-and-analysis">Step 4: Evaluation and analysis</h2> <h3 id="predicting-the-next-word">Predicting the next word</h3> <p>Take some example context window and use the model to predict the next word.</p> <ul> <li>Apply the model to the integer-encoded context window. As usual, this gives you (the logits of) a probability distribution over your vocabulary.</li> <li>Use <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html" rel="external nofollow noopener" target="_blank"><code>argmax</code></a> to find the index of the highest-scoring item, or <a href="https://pytorch.org/docs/stable/generated/torch.topk.html" rel="external nofollow noopener" target="_blank"><code>topk</code></a> to find the indices and scores of the <em>k</em> highest-scoring items.</li> <li>Apply the inverse vocabulary encoder (that you created in Step 2) so that you can understand what words the model thinks are the most likely in this context.</li> </ul> <h3 id="quantitative-evaluation">Quantitative evaluation</h3> <p>The most common way to evaluate language models quantitatively is the <a href="https://huggingface.co/docs/transformers/perplexity" rel="external nofollow noopener" target="_blank">perplexity</a> score on a test dataset. The better the model is at predicting the actually occurring words, the lower the perplexity. This quantity is formally defined as follows:</p> \[\text{perplexity} = 2^{-\frac{1}{m}\sum_{i=1}^m \log_2 P(w_i | c_i)}\] <p>In this formula, <em>m</em> is the number of words in the dataset, <em>P</em> is the probability assigned by our model, <em>w<sub>i</sub></em> and <em>c<sub>i</sub></em> the word and context window at each position.</p> <p>Compute the perplexity of your model on the validation set. The exact value will depend on various implementation choices you have made, how much of the training data you have been able to use, etc. Roughly speaking, if you get perplexity scores around 700 or more, there are probably problems. Carefully implemented and well-trained models will probably have perplexity scores in the range of 200–300.</p> <details> <summary><b>Hint</b>: An easy way to compute the perplexity in PyTorch.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> As you can see in the formula, the perplexity is an exponential function applied to the mean of the negative log probability of each token. You are probably already computing the <em>cross-entropy loss</em> as part of your training loop, and this actually computes what you need here. The perplexity is traditionally defined in terms of logarithms of base 2. However, we will get the same result regardless of what logarithmic base we use. So it is OK to use the natural logarithms and exponential functions, as long as we are consistent: this means that we can compute the perplexity by applying <code>exp</code> to the mean of the cross-entropy loss over your batches in the validation set. </div> </details> <p>If you have time for exploration, investigate the effect of the context window size <em>N</em> (and possibly other hyperparameters such as embedding dimensionality) on the model’s perplexity.</p> <h3 id="inspecting-the-word-embeddings">Inspecting the word embeddings</h3> <p>It is common to say that neural networks are “black boxes” and that we cannot fully understand their internal mechanics, especially as they grow larger and structurally more complex. The research area of model interpretability aims to develop methods to help us reason about the high-level functions the models implement.</p> <p>In this assignment, we will briefly investigate the <a href="https://en.wikipedia.org/wiki/Word_embedding" rel="external nofollow noopener" target="_blank">embeddings</a> that your model learned while you trained it. If we have successfully trained a word embedding model, an embedding vector stores a crude representation of “word meaning”, so we can reason about the learned meaning representations by investigating the geometry of the vector space of word embeddings. The most common way to do this is to look at nearest neighbors in the vector space: intuitively, if we look at some example word, its neighbors should correspond to words that have a similar meaning.</p> <p>Select some example words (e.g. <code class="language-plaintext highlighter-rouge">"sweden"</code>) and look at their nearest neighbors in the vector space of word embeddings. Does it seem that the nearest neighbors make sense?</p> <details> <summary><b>Hint</b>: Example code for computing nearest neighbors.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> The following code shows how to compute the nearest neighbors in the embedding space of a given word. Depending on your implementation, you may need to change some details. Here, <code>emb</code> is the <code>nn.Embedding</code> module of your language model, while <code>voc</code> and <code>inv_voc</code> are the string-to-integer and integer-to-string mappings you created in Step 2. <pre>
def nearest_neighbors(emb, voc, inv_voc, word, n_neighbors=5):

    # Look up the embedding for the test word.
    test_emb = emb.weight[voc[word]]
    
    # We'll use a cosine similarity function to find the most similar words.
    sim_func = nn.CosineSimilarity(dim=1)
    cosine_scores = sim_func(test_emb, emb.weight)
    
    # Find the positions of the highest cosine values.
    near_nbr = cosine_scores.topk(n_neighbors+1)
    topk_cos = near_nbr.values[1:]
    topk_indices = near_nbr.indices[1:]
    # NB: the first word in the top-k list is the query word itself!
    # That's why we skip the first position in the code above.
    
    # Finally, map word indices back to strings, and put the result in a list.
    return [ (inv_voc[ix.item()], cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]
</pre> </div> </details> <p>Optionally, you may visualize some word embeddings in a two-dimensional plot.</p> <details> <summary><b>Hint</b>: Example code for PCA-based embedding scatterplot.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <pre>
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
def plot_embeddings_pca(emb, inv_voc, words):
    vectors = np.vstack([emb.weight[inv_voc[w]].cpu().detach().numpy() for w in words])
    vectors -= vectors.mean(axis=0)
    twodim = TruncatedSVD(n_components=2).fit_transform(vectors)
    plt.figure(figsize=(5,5))
    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')
    for word, (x,y) in zip(words, twodim):
        plt.text(x+0.02, y, word)
    plt.axis('off')

plot_embeddings_pca(model[0], prepr, ['sweden', 'denmark', 'europe', 'africa', 'london', 'stockholm', 'large', 'small', 'great', 'black', '3', '7', '10', 'seven', 'three', 'ten', '1984', '2005', '2010'])
</pre> </div> </details> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?028b9776046506bdc87e88640817bd78"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>