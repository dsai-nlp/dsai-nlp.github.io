<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 1 | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment1/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-1">DAT450/DIT247: Programming Assignment 1</h1> <p>Our goal in this assignment is to implement a neural network-based language model similar to the one described by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">Bengio et al. (2003)</a>.</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>Download the following text files. They consist of Wikipedia articles converted into raw text.</p> <h2 id="step-1-preprocessing-the-text">Step 1: Preprocessing the text</h2> <p>You will need a <em>tokenizer</em> that splits English text into separate words (tokens). In this assignment, you will just use an existing tokenizer. Popular NLP libraries such as SpaCy and NLTK come with built-in tokenizers. We recommend NLTK in this assignment since it is somewhat faster than SpaCy and somewhat easier to use.</p> <details> <summary><b>Hint</b>: how to use NLTK's English tokenizer</summary> Import the function <code>word_tokenize</code> from the <code>nltk</code> library. If you are running this on your own machine, you will first need to install NLTK with <code>pip</code> or <code>conda</code>. In Colab, NLTK is already installed. For instance, <code>word_tokenize("Let's test!!")</code> should give the result <code>["Let", "'s", "test", "!", "!"]</code> </details> <p>Apply the tokenizer to all paragraphs in the training and validation datasets. Convert all words into lowercase.</p> <p><strong>Sanity check</strong>: after this step, your training set should consist of around 147,000 paragraphs and the validation set around 18,000 paragraphs. (The exact number depends on what tokenizer you selected.)</p> <h2 id="step-2-encoding-the-text-as-integers">Step 2: Encoding the text as integers</h2> <h3 id="building-the-vocabulary">Building the vocabulary</h3> <p>Create a utility (a function or a class) that goes through the training text and creates a <em>vocabulary</em>: a mapping from token strings to integers.</p> <p>In addition, the vocabulary should contain 3 special symbols:</p> <ul> <li>a symbol for previously unseen or low-frequency tokens,</li> <li>a symbol we will put at the beginning of each paragraph,</li> <li>a symbol we will put at the end of each paragraph.</li> </ul> <p>The total size of the vocabulary (including the 3 symbols) should be at most <code class="language-plaintext highlighter-rouge">max_voc_size</code>, which is is a user-specified hyperparameter. If the number of unique tokens in the text is greater than <code class="language-plaintext highlighter-rouge">max_voc_size</code>, then use the most frequent ones.</p> <details> <summary><b>Hint</b>: A <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="external nofollow noopener" target="_blank"><code>Counter</code></a> can be convenient when computing the frequencies (click to expand for more explanation).</summary> A <code>Counter</code> is like a regular Python dictionary, with some additional functionality for computing frequencies. For instance, you can go through each paragraph and call <a href="https://docs.python.org/3/library/collections.html#collections.Counter.update" rel="external nofollow noopener" target="_blank"><code>update</code></a>. After building the <code>Counter</code> on your dataset, <a href="https://docs.python.org/3/library/collections.html#collections.Counter.most_common" rel="external nofollow noopener" target="_blank"><code>most_common</code></a> gives the most frequent items. </details> <p>Also create some tool that allows you to go back from the integer to the original word token. This will only be used in the final part of the assignment, where we use the model to predict the next word.</p> <p><strong>Sanity check</strong>: after creating the vocabulary, make sure that</p> <ul> <li>the size of your vocabulary is not greater than the max vocabulary size you specified,</li> <li>the 3 special symbols exist in the vocabulary and that they don’t coincide with any real words,</li> <li>some highly frequent example words (e.g. “the”, “and”) are included in the vocabulary but that some rare words (e.g. “cuboidal”, “epiglottis”) are not.</li> <li>if you take some test word, you can map it to an integer and then back to the original test word using the inverse mapping.</li> </ul> <h3 id="encoding-the-texts-and-creating-training-instances">Encoding the texts and creating training instances</h3> <p>We will now collect training instances for our language model, where we learn to predict the next token given the previous <em>N</em> tokens.</p> <p>Go through the training and validation data and extract all sequences of <em>N</em>+1 tokens and map them to the corresponding integer values. Remember to use the special symbols when necessary:</p> <ul> <li>the “unseen” symbol for tokens not in your vocabulary,</li> <li> <em>N</em> “beginning” symbols before each paragraph,</li> <li>an “end” symbol after each paragraph.</li> </ul> <p>Store all these sequences in lists.</p> <p><strong>Sanity check</strong>: after these steps, you should have around 12 million training instances and 1.5 million validation instances.</p> <h2 id="step-3-developing-a-language-model">Step 3: Developing a language model</h2> <h3 id="setting-up-the-neural-network-structure">Setting up the neural network structure</h3> <p>Set up a neural network inspired by the neural language model proposed by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">Bengio et al. (2003)</a>. The main components are:</p> <ul> <li>an <em>embedding layer</em> that maps token integers to floating-point vectors,</li> <li> <em>intermediate layers</em> that map between input and output representations,</li> <li>an <em>output layer</em> that computes (the logits of) a probability distribution over the vocabulary.</li> </ul> <p>You are free to experiment with the design of the intermediate layers and you don’t have to follow the exact structure used in the paper.</p> <details> <summary> <b>Side note</b>: Setting up a neural network in PyTorch (click to expand)</summary> There are a few different ways that we can write code to set up a neural network in PyTorch. If your model has the traditional structure of stacked layers, then the most concise way to declare the model is to use `nn.Sequential`: ``` model = nn.Sequential( layer1, layer2, ... layerN) ``` You can use any type of layers here. In our case, you'll typically start with a `nn.Embedding` layer, followed by some intermediate layers (e.g. `nn.Linear` followed by some activation such as `nn.ReLU`), and then a linear output layer. A more general solution is to declare your network as a class that inherits from `nn.Module`. You will then have to declare your model components in `__init__` and define the forward computation in `forward`: ``` class MyNetwork(nn.Module): def __init__(self, hyperparameters): super().__init__() self.layer1 = ... some layer ... self.layer2 = ... some layer ... ... def forward(self, inputs): step1 = self.layer1(inputs) step2 = self.layer2(step1) ... return something ``` The second coding style, while more verbose, has the advantage that it is easier to debug: for instance, it is easy to check the shapes of intermediate computations. It is also more flexible and allows you to go beyond the constraints of a traditional layered setup. </details> <p><strong>Hint</strong>: a <code class="language-plaintext highlighter-rouge">nn.Flatten</code> layer is a convenient tool that you can put after the embedding layer to get the right tensor shapes. Let’s say we have a batch of <em>B</em> inputs, each of which is a context window of size <em>N</em>, so our input tensor has the shape (<em>B</em>, <em>N</em>). The output from the embedding layer will have the shape (<em>B</em>, <em>N</em>, <em>D</em>) where <em>D</em> is the embedding dimensionality. If you use a <code class="language-plaintext highlighter-rouge">nn.Flatten</code>, we go back to a two-dimensional tensor of shape (<em>B</em>, <em>N</em> * <em>D</em>). That is, we can see this as a step that concatenates the embeddings of the tokens in the context window.</p> <p><strong>Sanity check</strong>: carry out the following steps:</p> <ul> <li>Create an integer tensor of shape 1x<em>N</em> where <em>N</em> is the size of the context window. It doesn’t matter what the integers are except that they should be less than the vocabulary size. (Alternatively, take one instance from your training set.)</li> <li>Apply the model to this input tensor. It shouldn’t crash here.</li> <li>Make sure that the shape of the returned output tensor is 1x<em>V</em> where <em>V</em> is the size of the vocabulary. This output corresponds to the logits of the next-token probability distribution, but it is useless at this point because we haven’t yet trained the model.</li> </ul> <h3 id="training-the-model">Training the model</h3> <p><strong>Hint</strong>: while developing the code, work with very small datasets until you know it doesn’t crash. Monitor the cross-entropy loss (and/or the perplexity) over the training: if the loss does not decrease while you are training, there is probably an error.</p> <h2 id="step-4-evaluation-and-analysis">Step 4: Evaluation and analysis</h2> <h3 id="predicting-the-next-word">Predicting the next word</h3> <p>Take some example context window and use the model to predict the next word.</p> <h3 id="quantitative-evaluation">Quantitative evaluation</h3> <p>Compute the <a href="https://huggingface.co/docs/transformers/perplexity" rel="external nofollow noopener" target="_blank">perplexity</a> of your model on the validation set.</p> <p><strong>Hint</strong>: the perplexity is <code class="language-plaintext highlighter-rouge">exp</code> applied to the mean of the negative log probability of each token. The cross-entropy loss can be practical here, since it computes the mean negative log probability.</p> <h3 id="inspecting-the-word-embeddings">Inspecting the word embeddings</h3> <p>Testing.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>