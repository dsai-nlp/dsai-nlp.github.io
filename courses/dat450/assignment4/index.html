<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 4: Comparing fine-tuning methods | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="NLP@DSAI is a constellation of researchers who carry out foundational or applied research in natural language processing (NLP), or are interested in NLP techniques generally. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment4/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-4-comparing-fine-tuning-methods">DAT450/DIT247: Programming Assignment 4: Comparing fine-tuning methods</h1> <p>In this assignment, you will fine-tune a pre-trained Transformer model for a <em>classification</em> task: sentiment analysis of movie reviews.</p> <h3 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h3> <ul> <li>You will see how the accuracy and computational efficiency are affected by the different fine-tuning methods.</li> <li>You will learn about the LoRA method for parameter-efficient fine-tuning.</li> <li>You will get some practical experience of working with HuggingFace libraries, which provide useful utilities for preprocessing and training.</li> </ul> <h3 id="requirements">Requirements</h3> <p>Please submit your solution in <a href="https://chalmers.instructure.com/courses/31739/assignments/98456" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline</strong>: December 6.</p> <p>Submit a notebook containing your solution to the programming tasks described below. This is a pure programming assignment and you do not have to write a technical report or explain details of your solution in the notebook: there will be a separate individual assignment where you will answer some conceptual questions about what you have been doing here.</p> <h3 id="acknowledgement">Acknowledgement</h3> <p>This assignment is a lightly modified version of a similar assignment by Marco Kuhlmann.</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <h3 id="libraries">Libraries</h3> <p>In this assignment, we will rely on a set of libraries from the <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">HuggingFace</a> community:</p> <ul> <li><a href="https://huggingface.co/docs/transformers/index" rel="external nofollow noopener" target="_blank">Transformers</a></li> <li><a href="https://huggingface.co/docs/datasets/index" rel="external nofollow noopener" target="_blank">Datasets</a></li> <li><a href="https://huggingface.co/docs/evaluate/en/index" rel="external nofollow noopener" target="_blank">Evaluate</a></li> </ul> <p>Make sure all libraries are installed in your environment. If you use Colab, you will need to install Datasets and Evaluate, while Transformers is included in the pre-installed environment.</p> <h3 id="getting-the-files">Getting the files</h3> <p>The data we use in this assignment is a subset of the <a href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="external nofollow noopener" target="_blank">Large Movie Review Dataset</a>. The full dataset consists of 50,000 highly polar movie reviews collected from the Internet Movie Database (IMDB). We use a random sample consisting of 2,000 reviews for training and 500 reviews for evaluation.</p> <p>Download <a href="https://www.cse.chalmers.se/~richajo/dat450/data/pa4/pa4.zip" rel="external nofollow noopener" target="_blank">this zip file</a>, which contains the training and evaluation CSV files.</p> <h2 id="step-1-full-fine-tuning">Step 1: Full fine-tuning</h2> <p>In this assignment, we will use a compressed version of BERT called <a href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="external nofollow noopener" target="_blank">DistilBERT</a>. Weâ€™ll use the uncased version: that is, the tokenizer will not distinguish uppercase and lowercase.</p> <p>In the HuggingFace utilities that require you to specify a model name, you should use <code class="language-plaintext highlighter-rouge">distilbert-base-uncased</code>.</p> <h3 id="preprocessing">Preprocessing</h3> <p><a href="https://huggingface.co/docs/datasets/create_dataset" rel="external nofollow noopener" target="_blank">Create a Dataset</a> by loading the training and evaluation CSV files you previously downloaded.</p> <details> <summary><b>Hint</b>: Creating a Dataset.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <pre>
from datasets import load_dataset
imdb_dataset = load_dataset('csv', data_files = {'train': 'path/to/train.csv', 'eval': 'path/to/eval.csv'})
</pre> </div> </details> <p>Load the pre-trained tokenizer using <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> and apply it to the Dataset.</p> <details> <summary><b>Hint</b>: Applying a tokenizer to a Dataset.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> It's easiest if we create a helper function that applies the tokenizer to the right Dataset columns and with the right parameters. <pre>
def tokenize_helper(batch):
    return tokenizer(batch['review'], padding=True, truncation=True)
tokenized_imdb_dataset = imdb_dataset.map(tokenize_helper, batched=True)
</pre> This step will create new Dataset columns `input_ids` and `attention_mask`. </div> </details> <p><strong>Note</strong>: you may receive some warnings caused by parallelism in the tokenizer. To get rid of the warnings, you can use the following workaround.</p> <pre>
import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
</pre> <h3 id="creating-your-classification-model-for-fine-tuning">Creating your classification model for fine-tuning</h3> <p>Use the HuggingFace utility <code class="language-plaintext highlighter-rouge">AutoModelForSequenceClassification</code> to set up a model that you can fine-tune. Use the <code class="language-plaintext highlighter-rouge">from_pretrained</code> method with the model name set as above, and <code class="language-plaintext highlighter-rouge">num_labels=2</code> (because we have two-class classification task). This method carries out the following steps:</p> <ul> <li>It loads the pre-trained DistilBERT model from the HuggingFace repository (or from a cached file, if you have used the model before).</li> <li>It sets up untrained layers to map from the DistilBERT output to the two class labels. They will be trained during the fine-tuning process below.</li> </ul> <p><strong>Sanity check</strong>: Print the model in a notebook cell. You should see a visual representation of layers the model consists of. You should see the DistilBERT model including embedding layers and Transfomer layers. At the bottom of the list of layers, you should see two layers called <code class="language-plaintext highlighter-rouge">pre_classifier</code> and <code class="language-plaintext highlighter-rouge">classifier</code>, which are the newly created classification layers.</p> <h3 id="counting-the-number-of-trainable-parameters">Counting the number of trainable parameters</h3> <p>Define a function <code class="language-plaintext highlighter-rouge">count_trainable_parameters</code> that computes the number of floating-point numbers that a given model will update during training.</p> <ul> <li>The methods <code class="language-plaintext highlighter-rouge">.parameters()</code> and <code class="language-plaintext highlighter-rouge">.named_parameters()</code> return a sequence of tensors containing the model parameters.</li> <li>When counting the <strong>trainable</strong> parameters, you should only include those tensors where <code class="language-plaintext highlighter-rouge">requires_grad</code> is <code class="language-plaintext highlighter-rouge">True</code>. That is: we want to exclude tensors containing parameters we will not update during training.</li> </ul> <p><strong>Sanity check</strong>: The number of trainable parameters for the model above should be 66955010.</p> <h3 id="preparing-for-training">Preparing for training</h3> <p>The class <code class="language-plaintext highlighter-rouge">TrainingArguments</code> defines some parameters controlling the training process. Weâ€™ll mostly use default values here. You only need to set the following parameters:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">output_dir</code>: the name of some directory where the <code class="language-plaintext highlighter-rouge">Trainer</code> will keep its file.</li> <li> <code class="language-plaintext highlighter-rouge">num_train_epochs</code>: the number of training epochs.</li> <li> <code class="language-plaintext highlighter-rouge">eval_strategy</code>: set this to <code class="language-plaintext highlighter-rouge">epoch</code> to see evaluation scores after each epoch.</li> </ul> <p>In addition, we need to define a helper function that will be used for evaluation after each epoch. We use a utility from the Evaluate library for this:</p> <pre>
import evaluate

accuracy_scorer = evaluate.load('accuracy')

def evaluation_helper(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy_scorer.compute(predictions=predictions, references=labels)
</pre> <h3 id="training-the-model">Training the model</h3> <p>Import <code class="language-plaintext highlighter-rouge">Trainer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library. Create a <code class="language-plaintext highlighter-rouge">Trainer</code> using the following arguments:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">model</code>: the model that you are fine-tuning;</li> <li> <code class="language-plaintext highlighter-rouge">args</code>: the training arguments you defined above;</li> <li> <code class="language-plaintext highlighter-rouge">train_dataset</code>: the <code class="language-plaintext highlighter-rouge">train</code> section of your tokenized <code class="language-plaintext highlighter-rouge">Dataset</code>;</li> <li> <code class="language-plaintext highlighter-rouge">eval_dataset</code>: the <code class="language-plaintext highlighter-rouge">eval</code> section of your tokenized <code class="language-plaintext highlighter-rouge">Dataset</code>;</li> <li> <code class="language-plaintext highlighter-rouge">compute_metrics</code>: the evaluation helper function you defined above.</li> </ul> <p>Run the fine-tuning process by calling <code class="language-plaintext highlighter-rouge">train()</code> on your <code class="language-plaintext highlighter-rouge">Trainer</code>. This will train for the specified number of epochs, computing loss and accuracy after each epoch.</p> <p>After training, you may call <code class="language-plaintext highlighter-rouge">save_model</code> on the <code class="language-plaintext highlighter-rouge">Trainer</code> to save the modelâ€™s parameters. In this way, you can reload it later without having to retrain it.</p> <details> <summary><b>Hint</b>: Avoiding accidental model reuse.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> It is probably a good idea to re-create the model (using <code>AutoModelForSequenceClassification.from_pretrained</code>) before each time you train it. Otherwise, you may accidentally train a model that has already been trained. </div> </details> <h2 id="step-2-tuning-the-final-layers-only">Step 2: Tuning the final layers only</h2> <p>Even with a minimal model such as DistilBERT, fine-tuning the full model is rather time-consuming. We will now consider fine-tuning approaches where we only work with a subset of the modelâ€™s parameters.</p> <p>Set up the model once again. Disable gradient computation for all parameter tensors except those that are trained from scratch. That is: the two layers in the classification head will be updated during training, while the DistilBERT model will be kept fixed.</p> <p><strong>Sanity check</strong>: The number of trainable parameters for this model should be 592130.</p> <details> <summary><b>Hint</b>: Avoiding accidental model reuse, again!</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> Once again, we recommend that you re-create the model using <code>AutoModelForSequenceClassification.from_pretrained</code> before this step, so that you don't accidentelly work with the model that you fine-tuned in Step 1. </div> </details> <details> <summary><b>Hint</b>: How to disable gradient computation for a parameter tensor.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> For a parameter tensor in a model, we can set the attribute <code>requires_grad</code> to <code>False</code>, which means that during backpropagation, gradients will not be computed with respect to these parameters. So the training process will not change these parameters. </p> <p> To find the parameter tensors to switch off, you can either 1) go into the <code>distilbert</code> component and iterate through its <code>parameters</code>, or 2) go through all the model's named parameters, and switch off all parameter tensors except <code>classifier</code> and <code>pre_classifier</code>. </p> </div> </details> <p>Train this model and compare the training speed and classification accuracy to the results from Step 1.</p> <h2 id="step-3-fine-tuning-with-lora">Step 3: Fine-tuning with LoRA</h2> <h3 id="utilities-for-modifying-models">Utilities for modifying models</h3> <p>Define a function <code class="language-plaintext highlighter-rouge">extract_qv_layers</code> that extracts the query and value linear layers from all Transformer blocks in a DistilBERT model. Return a dictionary that maps the component name to the corresponding linear layer.</p> <details> <summary><b>Hint</b>: How to access the query and value linear layers.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> As we saw earlier, the DistilBERT model consists of a hierarchy of nested submodules. Each of these can be addressed by a fully-qualified string name. </p> <p> You can use get_submodule() to retrieve a layer by a string name. For instance, <code>'distilbert.transformer.layer.0.attention.q_lin'</code> refers to the Q part of Transformer layer 0. </p> <p> It's OK to hard-code this part, so that you just enumerate the Q and V parts of all layers here. </p> </div> </details> <p><strong>Sanity check</strong>: If you apply this on a DistilBERT model, the result should contain 12 named linear layers.</p> <p>We also need a convenience function that puts layers back into a model. The following function does the trick. The <code class="language-plaintext highlighter-rouge">named_layers</code> argument uses the same format as returned by <code class="language-plaintext highlighter-rouge">extract_qv_layers</code>.</p> <pre>
def replace_layers(model, named_layers):
    for name, layer in named_layers.items():
        components = name.split('.')
        submodule = model
        for component in components[:-1]:
            submodule = getattr(submodule, component)
        setattr(submodule, components[-1], layer)
</pre> <h3 id="implementing-the-lora-layer">Implementing the LoRA layer</h3> <p>To implement the LoRA approach, we define a new type of layer that will be used as a drop-in replacement for a regular linear layer.</p> <p>In <a href="https://arxiv.org/pdf/2106.09685" rel="external nofollow noopener" target="_blank">the paper by Hu et al. (2021)</a>, the structure is presented visually in Figure 1, and equation (3) shows the same idea.</p> <p>Start from the following skeleton and fill in the missing pieces:</p> <pre>
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, W, r, alpha):
        super().__init__()
        # TODO: Add your code here

    def forward(self, x):
        # TODO: Replace the next line with your own code
        raise NotImplementedError
</pre> <p>Here, <code class="language-plaintext highlighter-rouge">W</code> is the linear layer we are fine-tuning, while <code class="language-plaintext highlighter-rouge">r</code> and <code class="language-plaintext highlighter-rouge">alpha</code> are hyperparameters described in section 4.1. of the paper. The <code class="language-plaintext highlighter-rouge">r</code> parameter controls the parameter efficiency: by setting it to a low value, we save memory but make a rougher approximation. The <code class="language-plaintext highlighter-rouge">alpha</code> parameter is a scaling factor.</p> <details> <summary><b>Hint</b>: How to initialize <code>A</code> and <code>B</code>.</summary> <div style="margin-left: 10px; border-radius: 4px; background: #ddfff0; border: 1px solid black; padding: 5px;"> <p> To follow the description closely, we should use the parameter initialization approach recommended in the paper (see Figure 1). </p> <p> You can use <code>nn.init.normal_</code> and <code>nn.init.zeros_</code> here. </p> </div> </details> <h3 id="fine-tuning-with-lora">Fine-tuning with LoRA</h3> <p>Set up a model where you replace the query and value linear layers with LoRA layers. Use the following steps:</p> <ul> <li>First use <code class="language-plaintext highlighter-rouge">extract_qv_layers</code> to get the relevant linear layers.</li> <li>Each of the linear layers in the returned dictionary should be wrapped inside a LoRA layer.</li> <li>Then use <code class="language-plaintext highlighter-rouge">replace_layers</code> to put them back into the model.</li> </ul> <p><strong>Sanity check</strong>: Use your function <code>count_trainable_parameters</code>. The number of trainable parameters should be less than in Step 1 but more than in Step 2. The exact number will depend on the rank.</p> <p>Train this model and compare the training speed and classification accuracy to the results from Steps 1 and 2.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>