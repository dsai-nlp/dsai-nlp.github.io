<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DAT450/DIT247: Programming Assignment 5: Retrieval-augmented text generation | NLP@DSAI</title> <meta name="author" content=" "> <meta name="description" content="NLP@DSAI is a constellation of researchers who carry out foundational or applied research in natural language processing (NLP), or are interested in NLP techniques generally. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/custom.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dsai-nlp.github.io/courses/dat450/assignment5/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">NLP@DSAI</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">NLP@DSAI</a> </li> <li class="nav-item "> <a class="nav-link" href="/members">Members</a> </li> <li class="nav-item "> <a class="nav-link" href="/events">Events</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/courses/">Courses</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <h1 id="dat450dit247-programming-assignment-5-retrieval-augmented-text-generation">DAT450/DIT247: Programming Assignment 5: Retrieval-augmented text generation</h1> <p>In this assignment we will build our own RAG pipeline using LangChain.</p> <h2 id="pedagogical-purposes-of-this-assignment">Pedagogical purposes of this assignment</h2> <ul> <li>Get an understanding of how RAG can be used within NLP.</li> <li>Learn how to use LangChain to build NLP applications.</li> <li>Get an understanding for the challenges and use cases of RAG.</li> </ul> <h2 id="requirements">Requirements</h2> <p>Please submit your solution in <a href="https://canvas.chalmers.se/courses/36909/assignments/117619" rel="external nofollow noopener" target="_blank">Canvas</a>. <strong>Submission deadline: December 8.</strong></p> <p>Submit Python files containing your solution to the programming tasks described below. In addition, to save time for the people who grade your submission, please submit a text file containing the outputs printed out by your Python program; read the instructions carefully so that the right outputs are included.</p> <p>This is a pure programming assignment and you do not have to write a technical report: there will be a separate individual assignment where you will answer some conceptual questions about what you have been doing here.</p> <h2 id="step-0-preliminaries">Step 0: Preliminaries</h2> <p>For the assignments in the course, you can use an environment we have prepared for this course: <code class="language-plaintext highlighter-rouge">/data/courses/2025_dat450_dit247/venvs/dat450_venv</code>. (So to activate this environment, you type <code class="language-plaintext highlighter-rouge">source /data/courses/2025_dat450_dit247/venvs/dat450_venv/bin/activate</code>.)</p> <p>If you are running on Colab or your own environment, make sure the following packages are installed:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>langchain 
pip <span class="nb">install </span>langchain-community
pip <span class="nb">install </span>langchain-huggingface
pip <span class="nb">install </span>langchain-core
pip <span class="nb">install </span>sentence_transformers
pip <span class="nb">install </span>langchain-chroma
</code></pre></div></div> <h2 id="step-1-get-the-dataset">Step 1: Get the dataset</h2> <p>You will be working with the <a href="https://github.com/pubmedqa/pubmedqa" rel="external nofollow noopener" target="_blank">PubMedQA dataset</a> described in this <a href="https://aclanthology.org/D19-1259.pdf" rel="external nofollow noopener" target="_blank">paper</a>. The dataset has been created based on medical research papers from <a href="https://pubmed.ncbi.nlm.nih.gov/" rel="external nofollow noopener" target="_blank">PubMed</a>, you can read more about it in the linked paper.</p> <p>Use the following code to get the dataset for the assignment.</p> <p>If you are running on Minerva or your own environment, run the following command in your command line. Otherwise if you are using notebook e.g. Colab, you can write the following command in a code block with an extra <code class="language-plaintext highlighter-rouge">!</code> before and run the code block.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://raw.githubusercontent.com/pubmedqa/pubmedqa/refs/heads/master/data/ori_pqal.json
</code></pre></div></div> <h3 id="collect-two-datasets">Collect two datasets</h3> <p>You will collect two datasets from the downloaded file:</p> <ul> <li>‘questions’: the questions with corresponding gold long answer, gold document ID, and year.</li> <li>‘documents’: the abstracts (contexts+long_answer concatenated), and year.</li> </ul> <p>You can run the following codes to collet these two datasets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">tmp_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_json</span><span class="p">(</span><span class="sh">"</span><span class="s">ori_pqal.json</span><span class="sh">"</span><span class="p">).</span><span class="n">T</span>
<span class="c1"># some labels have been defined as "maybe", only keep the yes/no answers
</span><span class="n">tmp_data</span> <span class="o">=</span> <span class="n">tmp_data</span><span class="p">[</span><span class="n">tmp_data</span><span class="p">.</span><span class="n">final_decision</span><span class="p">.</span><span class="nf">isin</span><span class="p">([</span><span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">])]</span>

<span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">abstract</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">).</span><span class="nf">join</span><span class="p">(</span><span class="n">row</span><span class="p">.</span><span class="n">CONTEXTS</span><span class="o">+</span><span class="p">[</span><span class="n">row</span><span class="p">.</span><span class="n">LONG_ANSWER</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
             <span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="n">YEAR</span><span class="p">})</span>
<span class="n">questions</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="n">QUESTION</span><span class="p">,</span>
             <span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="n">YEAR</span><span class="p">,</span>
             <span class="sh">"</span><span class="s">gold_label</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="n">final_decision</span><span class="p">,</span>
             <span class="sh">"</span><span class="s">gold_context</span><span class="sh">"</span><span class="p">:</span> <span class="n">tmp_data</span><span class="p">.</span><span class="n">LONG_ANSWER</span><span class="p">,</span>
             <span class="sh">"</span><span class="s">gold_document_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">documents</span><span class="p">.</span><span class="n">index</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Sanity check:</strong> You can print out some of the data in the dataset.</p> <p>An example of a question our RAG pipeline should answer:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>questions.iloc[0].question
</code></pre></div></div> <p>An example of a document the pipeline can leverage to answer the questions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>documents.iloc[0].abstract
</code></pre></div></div> <h2 id="step-2-configure-your-langchain-lm">Step 2: Configure your LangChain LM</h2> <h3 id="step-21-find-a-language-model-from-huggingface">Step 2.1: Find a language model from HuggingFace</h3> <p>Define a language model that will act as the generative model in your RAG pipeline. You can browse for different Hugging Face models on their <a href="https://huggingface.co/models" rel="external nofollow noopener" target="_blank">webpage</a>.</p> <blockquote> <p>Some interesting models (e.g. Llama 3.2) may require that you apply for access. This process is usually quite fast, while it may require that you create an account on Hugging Face (it is free). To use a gated model you need to generate a personal HF token and put it as a secret in your notebook (if using Colab). Make sure that the token has enabled “Read access to contents of all public gated repos you can access”.</p> </blockquote> <details> <summary><b>Hint:</b> How to set up HuggingFace Token when using Minerva</summary> If you need to use the huggingface token and you are using Minerva, one way to do it is to add the global parameter in your bash file: `export HF_TOKEM = your_{token}`, and then refer to it in your python code: `hf_token = os.getenv('HF_token')`. Also, to avoid your token being misused, remember to remove the actual token you are using from your submission. </details> <h3 id="step-22-load-the-language-model">Step 2.2 Load the language model</h3> <p>You can load the HuggingFace language model using <code class="language-plaintext highlighter-rouge">HuggingFacePipeline.from_model_id</code></p> <p>When calling <code class="language-plaintext highlighter-rouge">HuggingFacePipeline</code>, set <code class="language-plaintext highlighter-rouge">return_full_text=False</code> to only return the assistant’s response, and call <code class="language-plaintext highlighter-rouge">model.invoke(your_prompt)</code> to retrieve the text of the output.</p> <p><strong>Sanity check:</strong> Prompt your LangChain model and confirm that it returns a reasonable output.</p> <p><strong>Include the prompt and the output of this model in your output file.</strong></p> <h2 id="step-3-set-up-the-document-database">Step 3: Set up the document database</h2> <h3 id="step-31-embedding-model">Step 3.1: Embedding model</h3> <p>First, you need a model to embed the documents in the retrieval corpus. Here, we recommend using the <a href="https://docs.langchain.com/oss/python/integrations/text_embedding/huggingfacehub" rel="external nofollow noopener" target="_blank">HuggingFaceEmbeddings</a> function.</p> <p><strong>Sanity check:</strong> Pass a text passage to the embedding model by calling <code class="language-plaintext highlighter-rouge">embed_query</code> and evaluate its shape. It should be of the shape (embedding_dim,).</p> <h3 id="step-32-chunking">Step 3.2: Chunking</h3> <p>Second, you need to chunk the documents in your retrieval corpus, as some likely are too long for the embedding model. Here, you can use the <a href="https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter" rel="external nofollow noopener" target="_blank">RecursiveCharacterTextSplitter</a> as a start. The retrieval corpus is given by <code class="language-plaintext highlighter-rouge">documents.abstract</code>, so you can use <code class="language-plaintext highlighter-rouge">create_documents</code> on the text splitter with the retrieval corpus to create LangChain <code class="language-plaintext highlighter-rouge">Document</code> objects, and then use <code class="language-plaintext highlighter-rouge">split_documents</code> to create text chunks that will be used in creating the vector store.</p> <p>For evaluation in Step 5, we recommend saving the document id as <code class="language-plaintext highlighter-rouge">metadatas</code> when creating the document:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metadatas</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">idx</span><span class="p">}</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">.</span><span class="n">index</span><span class="p">]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">create_documents</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="n">documents</span><span class="p">.</span><span class="n">abstract</span><span class="p">.</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadatas</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Sanity check:</strong> Print some samples from the text chunks and check that it makes sense. This way, you might be able to get a feeling for a good chunk size.</p> <h3 id="step-33-define-a-vector-store">Step 3.3: Define a vector store</h3> <p>Third, you need a vector store to store the documents and corresponding embeddings. There are many document databases and retrievers to play around with. As a start, you can use the <a href="https://python.langchain.com/docs/integrations/vectorstores/chroma/" rel="external nofollow noopener" target="_blank">Chroma</a> vector store with cosine similarity as the distance metric.</p> <p>When building your vector store, pass the embedding model in <a href="#step-31-embedding-model">Step 3.1</a> as the embedding model and use the text chunks in <a href="#step-32-chunking">Step 3.2</a> as the documents in the vector store. To add documents in the vector store, you can Use <code class="language-plaintext highlighter-rouge">Chroma.from_documents</code> when creating the vector store or use <code class="language-plaintext highlighter-rouge">vector_store.add_documents</code> after creating the vector store.</p> <p><strong>Sanity check:</strong> Query your vector store as follows and check that the results make sense:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="p">.</span><span class="nf">similarity_search_with_score</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">What is programmed cell death?</span><span class="sh">"</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">res</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">* [SIM=</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">] </span><span class="si">{</span><span class="n">res</span><span class="p">.</span><span class="n">page_content</span><span class="si">}</span><span class="s"> [</span><span class="si">{</span><span class="n">res</span><span class="p">.</span><span class="n">metadata</span><span class="si">}</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="step-4-define-the-full-rag-pipeline">Step 4: Define the full RAG pipeline</h2> <p>In this and the following steps, we will guadually build a RAG chain.</p> <p>There could be two options of building a RAG chain, and you can choose either <strong>one</strong> of them to build your own RAG:</p> <p><a href="#option-a-build-a-rag-agent-based-on-the-official-langchain-guide">Option A</a>: Build a RAG agent based on the official LangChain guide: <a href="https://docs.langchain.com/oss/python/langchain/rag" rel="external nofollow noopener" target="_blank">here</a>. Here we will use a two-step chain, in which we will run a search in the vector store, and incorporate the result as context for LLM queries.</p> <p><a href="#option-b-build-a-rag-chain-based-on-langchain-open-tutorial">Option B</a>: Build a RAG chain using LangChain Expression Language (LCEL) based on a LangChain Open Tutorial: <a href="https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/05-RunnableParallel.ipynb#scrollTo=635d8ebb" rel="external nofollow noopener" target="_blank">here</a>. Here we will use the <a href="https://reference.langchain.com/python/langchain_core/runnables/?h=runnablepara#langchain_core.runnables.base.RunnableParallel" rel="external nofollow noopener" target="_blank">RunnableParallel</a> class to build a RAG chain that will also return the retrieved document.</p> <h3 id="option-a-build-a-rag-agent-based-on-the-official-langchain-guide">Option A: Build a RAG agent based on the official LangChain guide</h3> <p>Here, we will define a custom prompt while incorporating the retrieval step.</p> <p>In order to access the documents retrieved, we can create the prompt in a way that it will <a href="https://docs.langchain.com/oss/python/langchain/rag#returning-source-documents" rel="external nofollow noopener" target="_blank">return the source documents</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="n">langchain_core.documents</span> <span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span> <span class="n">langchain.agents.middleware</span> <span class="kn">import</span> <span class="n">AgentMiddleware</span><span class="p">,</span> <span class="n">AgentState</span>


<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">AgentState</span><span class="p">):</span>
    <span class="n">context</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">RetrieveDocumentsMiddleware</span><span class="p">(</span><span class="n">AgentMiddleware</span><span class="p">[</span><span class="n">State</span><span class="p">]):</span>
    <span class="n">state_schema</span> <span class="o">=</span> <span class="n">State</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vector_store</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vector_store</span> <span class="o">=</span> <span class="n">vector_store</span>

    <span class="k">def</span> <span class="nf">before_model</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AgentState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">last_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get the user input query
</span>        <span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vector_store</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">last_message</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># search for documents
</span>
        <span class="n">docs_content</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">retrieved_docs</span><span class="p">)</span>  

        <span class="n">augmented_message_content</span> <span class="o">=</span> <span class="p">(</span>
            <span class="c1"># Put your prompt here
</span>        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="n">last_message</span><span class="p">.</span><span class="nf">model_copy</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">augmented_message_content</span><span class="p">})],</span>
            <span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieved_docs</span><span class="p">,</span>
        <span class="p">}</span>

</code></pre></div></div> <p>As a start, you might want to fetch only one document per prompt.</p> <details> <summary><b>Hint:</b> Prompt model for classification later</summary> In Step 5, we will be using the RAG agent to evaluate whether the model can correctly answer the questions with "Yes" or "No". For evaluation, you may want to prompt the model in a way that it will return only "Yes" or "No" or at least lead the answer with "Yes" or "No". </details> <p>We are now ready to create a RAG agent. In this step, we can use <code class="language-plaintext highlighter-rouge">create_agent</code> to build a RAG agent, and use a <code class="language-plaintext highlighter-rouge">RetrieveDocumentsMiddleware</code> object to act as the middleware.</p> <p><strong>Sanity check:</strong> Take a question from your dataset and check whether the model seems to retrieve a relevant document, and answer in a reasonable fashion.</p> <p>To print out the results prettily, you can use the solution given by Langchain:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">agent</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span> <span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">your_query</span><span class="p">}]},</span>
    <span class="n">stream_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">values</span><span class="sh">"</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">step</span><span class="p">[</span><span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">pretty_print</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Include the prompt and the output of this model in your output file.</strong></p> <h3 id="option-b-build-a-rag-chain-based-on-langchain-open-tutorial">Option B: Build a RAG chain based on LangChain Open Tutorial</h3> <p>Here, we will firstly define a retriever on the vector store to retrieve documents:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">()</span>
</code></pre></div></div> <p>As a start, you might want the retriever to fetch only one document per prompt.</p> <p>Then, define your template and use <code class="language-plaintext highlighter-rouge">ChatPromptTemplate.from_template</code> to create a Chat Prompt.</p> <p>With the retriever and the prompt, you should be able to define the RAG chain. In order to return the retrieved context as well as the answers for further evaluation, firstly we can define a <code class="language-plaintext highlighter-rouge">RunnableParallel</code> object that can take the context and the question, then we can define a chain that only generate text outputs like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Construct the retrieval chain
</span><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">model</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div></div> <p>Lastly, combine the <code class="language-plaintext highlighter-rouge">RunnableParallel</code> object with the chain using the <a href="https://reference.langchain.com/python/langchain_core/runnables/?h=runnablepara#langchain_core.runnables.base.RunnableParallel.assign" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">assign</code></a> method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rag_chain</span> <span class="o">=</span> <span class="n">runnable_parallel_object</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">answer</span><span class="o">=</span><span class="n">chain</span><span class="p">)</span>
</code></pre></div></div> <p>Then you should be able to access the retrieved documents with <code class="language-plaintext highlighter-rouge">answer["context"]</code>.</p> <p><strong>Sanity check:</strong> Take a question from your dataset and check whether the model seems to retrieve a relevant document, and answer in a reasonable fashion.</p> <p><strong>Include the prompt and the output of this model in your output file.</strong></p> <h2 id="step-5-evaluate-rag-on-the-dataset">Step 5: Evaluate RAG on the dataset</h2> <p>Here we will do 4 evaluation tasks to evaluate the RAG agent with the given dataset.</p> <ol> <li>Evaluate your full RAG pipeline on the medical questions (<code class="language-plaintext highlighter-rouge">questions.question</code>) and corresponding gold labels (<code class="language-plaintext highlighter-rouge">questions.gold_label</code>).</li> </ol> <p>Since the gold labels can be casted to a binary variable (yes/no) you may use the f1 and/or accuracy metrics.</p> <p>We expect the model to give answers of “Yes” or “No”, but it can happen that the model gives random answers. In this case, one way to perform the evaluation is to keep track of the number of valid answers and do evaluation only on the valid answers.</p> <ol> <li> <p>As a baseline, run the same LM without context and compare the performance of the two setups. You can use the same evaluation method as the previous RAG evaluation. Did the retrieval help?</p> </li> <li> <p>Also evaluate whether the gold documents are fetched for each question. You can compare the retrieved document id with the gold document with ID given by <code class="language-plaintext highlighter-rouge">questions.gold_document_id</code>.</p> </li> <li> <p>Also, inspect some retrieved documents and corresponding model answers. Does the pipeline seem to work as intended?</p> </li> </ol> <p><strong>Include the evaluation results in your output file.</strong></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?028b9776046506bdc87e88640817bd78"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>