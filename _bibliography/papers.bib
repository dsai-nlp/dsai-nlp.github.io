---
---
# 2024
@inproceedings{farahani2024deciphering,
    title = "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models",
    author = "Farahani, Mehrdad  and
      Johansson, Richard",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.943",
    pages = "16966--16977",
    pdf = {https://aclanthology.org/2024.emnlp-main.943.pdf},    
    abstract = "Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the Atlas approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \textit{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \textit{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant.",
}

@article{saynova2024fact,
  title = {Fact Recall, Heuristics or Pure Guesswork? {P}recise Interpretations of Language Models for Fact Completion},
  url = {https://arxiv.org/abs/2410.14405},
  journal = {arXiv preprint arXiv:2410.14405},
  author = {Saynova, Denitsa and Hagstr\"{o}m, Lovisa and Johansson, Moa and Johansson, Richard and Kuhlmann, Marco},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2410.14405},
}

@inproceedings{audinet2024can,
  author = {Nicolas Audinet de Pieuchon and Adel Daoud and Connor Jerzak and Moa Johansson and Richard Johansson},
  title = {Can Large Language Models (or Humans) Disentangle Text?},
  year = {2024},
  booktitle = {Proceedings of the 6th Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)},
  url = {https://aclanthology.org/2024.nlpcss-1.5/},
  address = {Mexico City, Mexico},
  pdf = {https://aclanthology.org/2024.nlpcss-1.5.pdf},
  abstract = {We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detectable to machine learning classifiers post-LLM-disentanglement. Furthermore, we find that human annotators also struggle to disentangle sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of disentanglement methods that achieve statistical independence in representation space.}
}

@inproceedings{johansson2024what,
    title = {What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?},
    author = {Richard Johansson},
    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},
    year = {2024},
    address = {Turin, Italy},
    url = {https://aclanthology.org/2024.lrec-main.1520/},
    pdf = {https://aclanthology.org/2024.lrec-main.1520.pdf},
    abstract = {We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.}
}

# 2023
@inproceedings{hagstrom-etal-2023-effect,
    title = "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
    author = {Hagstr{\"o}m, Lovisa  and
      Saynova, Denitsa  and
      Norlund, Tobias  and
      Johansson, Moa  and
      Johansson, Richard},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.332",
    pdf = "https://aclanthology.org/2023.emnlp-main.332.pdf",    
    doi = "10.18653/v1/2023.emnlp-main.332",
    pages = "5457--5476",
    abstract = "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer {``}Edinburgh{''} to {``}Anne Redpath passed away in X.{''} and {``}London{''} to {``}Anne Redpath{'}s life ended in X.{''} In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a passage retrieval database. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency but that retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
}
@inproceedings{doostmohammadi-etal-2023-surface,
    title = "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
    author = "Doostmohammadi, Ehsan  and
      Norlund, Tobias  and
      Kuhlmann, Marco  and
      Johansson, Richard",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.45",
    pdf = "https://aclanthology.org/2023.acl-short.45.pdf",    
    doi = "10.18653/v1/2023.acl-short.45",
    pages = "521--529",
    abstract = "Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.",
}
@inproceedings{norlund-etal-2023-generalization,
    title = "On the Generalization Ability of Retrieval-Enhanced Transformers",
    author = "Norlund, Tobias  and
      Doostmohammadi, Ehsan  and
      Johansson, Richard  and
      Kuhlmann, Marco",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.109",
    pdf = "https://aclanthology.org/2023.findings-eacl.109.pdf",    
    doi = "10.18653/v1/2023.findings-eacl.109",
    pages = "1485--1493",
    abstract = "Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown impressive results: off-loading memory from trainable weights to a retrieval database can significantly improve language modeling and match the performance of non-retrieval models that are an order of magnitude larger in size. It has been suggested that at least some of this performance gain is due to non-trivial generalization based on both model weights and retrieval. In this paper, we try to better understand the relative contributions of these two components. We find that the performance gains from retrieval to a very large extent originate from overlapping tokens between the database and the test data, suggesting less of non-trivial generalization than previously assumed. More generally, our results point to the challenges of evaluating the generalization of retrieval-augmented language models such as RETRO, as even limited token overlap may significantly decrease test-time loss. We release our code and model at \url{https://github.com/TobiasNorlund/retro}",
}
@inproceedings{farahani-johansson-2023-empirical,
    title = "An Empirical Study of Multitask Learning to Improve Open Domain Dialogue Systems",
    author = "Farahani, Mehrdad  and
      Johansson, Richard",
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.36",
    pdf = "https://aclanthology.org/2023.nodalida-1.36.pdf",    
    pages = "347--357",
    abstract = "Autoregressive models used to generate responses in open-domain dialogue systems often struggle to take long-term context into account and to maintain consistency over a dialogue. Previous research in open-domain dialogue generation has shown that the use of \textit{auxiliary tasks} can introduce inductive biases that encourage the model to improve these qualities. However, most previous research has focused on encoder-only or encoder/decoder models, while the use of auxiliary tasks in \textit{encoder-only} autoregressive models is under-explored. This paper describes an investigation where four different auxiliary tasks are added to small and medium-sized GPT-2 models fine-tuned on the PersonaChat and DailyDialog datasets. The results show that the introduction of the new auxiliary tasks leads to small but consistent improvement in evaluations of the investigated models.",
}

@inproceedings{saynova-etal-2023-class,
    title = "Class Explanations: the Role of Domain-Specific Content and Stop Words",
    author = "Saynova, Denitsa  and
      Bruinsma, Bastiaan  and
      Johansson, Moa  and
      Johansson, Richard",
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.12",
    pdf = "https://aclanthology.org/2023.nodalida-1.12.pdf",
    pages = "103--112",
    abstract = "We address two understudied areas related to explainability for neural text models. First, \textit{class explanations}. What features are descriptive across a class, rather than explaining single input instances? Second, the \textit{type of features} that are used for providing explanations. Does the explanation involve the statistical pattern of word usage or the presence of domain-specific content words? Here, we present a method to extract both class explanations and strategies to differentiate between two types of explanations {--} domain-specific signals or statistical variations in frequencies of common words. We demonstrate our method using a case study in which we analyse transcripts of political debates in the Swedish Riksdag.",
}

# 2022
@inproceedings{malik2022,
    title = {Controlling for Stereotypes in Multimodal Language Model Evaluation},
    author = {Manuj Malik and Richard Johansson},
    booktitle = {Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
    month = {sep},
    year = {2022},
    address = {Abu Dhabi},
    publisher = {Association for Computational Linguistics},
    pages = {263-271},
    url = {https://aclanthology.org/2022.blackboxnlp-1.21/},
    optgupid = {323885},
}

@incollection{johansson2022,
    title = {Coveting Your Neighbor's Wife: Using Lexical Neighborhoods in Substitution-based Word Sense Disambiguation},
    author = {Richard Johansson},
    booktitle = {LIVE and LEARN -- Festschrift in honor of Lars Borin},
    month = {sep},
    year = {2022},
    address = {Gothenburg, Sweden},
    publisher = {University of Gothenburg},
    pages = {61-66},
    editor = {Volodina, Elena and Dann\'{e}lls, Dana and Berdicevskis, Aleksandrs and Forsberg, Markus and Virk, Shafqat},
    url = {https://gupea.ub.gu.se/handle/2077/74254},
    pdf = {https://gupea.ub.gu.se/handle/2077/74254},    
    optgupid = {326360},
}

@inproceedings{raj2022,
    title = {Cross-modal Transfer Between Vision and Language for Protest Detection},
    author = {Ria Raj and Kajsa And\'{e}asson and Tobias Norlund and Richard Johansson and Aron Lagerberg},
    booktitle = {Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)},
    month = {sep},
    year = {2022},
    address = {Abu Dhabi},
    publisher = {Association for Computational Linguistics},
    pages = {45-50},
    url = {https://aclanthology.org/2022.case-1.8/},
    pdf = {https://aclanthology.org/2022.case-1.8.pdf},
    optgupid = {323886},
}

@inproceedings{hagstrom2022c,
    title = {Can We Use Small Models to Investigate Multimodal Fusion Methods?},
    author = {Hagstr\"{o}m, Lovisa and Norlund, Tobias and Johansson, Richard},
    booktitle = {Proceedings of the 2022 CLASP Conference on (Dis)embodiment},
    month = {sep},
    year = {2022},
    address = {Gothenburg, Sweden},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.clasp-1.5},
    pages = {45-50},
    pdf = {https://aclanthology.org/2022.clasp-1.5.pdf},
    optgupid = {318477},
}

@inproceedings{hagstrom2022b,
    title = {How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?},
    author = {Hagstr\"{o}m, Lovisa and Johansson, Richard},
    booktitle = {Proceedings of the 29th International Conference on Computational Linguistics (COLING)},
    year = {2022},
    address = {Gyeongju, Republic of Korea},
    pages = {5582-5596},
    url = {https://aclanthology.org/2022.coling-1.494/},
    pdf = {https://aclanthology.org/2022.coling-1.494.pdf},    
    optgupid = {319269},
    optpreprint = {https://arxiv.org/abs/2209.08982}
}

@inproceedings{hagstrom2022a,
    title = {What do Models Learn From Training on More Than Text? {M}easuring Visual Commonsense Knowledge},
    author = {Hagstr\"{o}m, Lovisa and Johansson, Richard},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
    month = may,
    year = {2022},
    address = {Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.acl-srw.19},
    pages = {252-261},
    pdf = {https://aclanthology.org/2022.acl-srw.19.pdf},
    optgupid = {316251},
}

@inproceedings{daoud2022,
    title = {Conceptualizing Treatment Leakage in Text-based Causal Inference},
    author = {Daoud, Adel and Jerzak, Connor and Johansson, Richard},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = {jul},
    year = {2022},
    address = {Seattle, United States},
    publisher = "Association for Computational Linguistics",
    url = {https://aclanthology.org/2022.naacl-main.413/},
    pdf = {https://aclanthology.org/2022.naacl-main.413.pdf},    
    optaclid = {2022.naacl-main.413},
    pages = {5638-5645},
    optpreprint = {http://arxiv.org/abs/2205.00465},
    optgupid = {317410},
}

@article{hagberg2022,
title = {Semi-supervised Learning with Natural Language Processing for Right Ventricle Classification in Echocardiography -- a Scalable Approach},
journal = {Computers in Biology and Medicine},
volume = {143},
pages = {105282},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105282},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522000749},
pdf = {https://www.sciencedirect.com/science/article/pii/S0010482522000749},
author = {Eva Hagberg and David Hagerman and Richard Johansson and Nasser Hosseini and Jan Liu and Elin Bj\"{o}rnsson and Jennifer Alv\'{e}n and Ola Hjelmgren},
optgupid = {314455},
}

# 2021

@InProceedings{norlund2021,
        Author = {Tobias Norlund and Lovisa Hagstr\"{o}m and Richard Johansson},
        title = {Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?},
        Year = {2021},
        booktitle = {Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
        optgupid = {309548},
        address = {Punta Cana, Dominican Republic},
        optmonth = nov,
        date = {November 11},
        pages = {149-162},
        url = {https://aclanthology.org/2021.blackboxnlp-1.10/},
        pdf = {https://aclanthology.org/2021.blackboxnlp-1.10.pdf},	
        optacllink = {https://aclanthology.org/2021.blackboxnlp-1.10/},
}

@InProceedings{hagstrom2021,
        Author = {Lovisa Hagstr\"{o}m and Richard Johansson},
        title = {Knowledge Distillation for {S}wedish {NER} models: A Search for Performance and Efficiency},
        Year = {2021},
        booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021)},
        optgupid = {305832},
        address = {Reykjav\'{i}k, Iceland},
        date = {May 31-June 2},
        pages = {124-134},
        url = {https://www.aclweb.org/anthology/2021.nodalida-main.13},
        pdf = {https://www.aclweb.org/anthology/2021.nodalida-main.13.pdf},	
        opttoappear = {false},
        optacllink = {https://www.aclweb.org/anthology/2021.nodalida-main.13},
}

# 2020
# 2019
# 2018
# 2017
# 2016
# 2015
# 2014
# 2013
# 2012
# 2011
# 2010
# 2009
# 2008
# 2007
# 2006
# 2005
# 2004

